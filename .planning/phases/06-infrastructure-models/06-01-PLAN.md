---
phase: 06-infrastructure-models
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - Dockerfile
  - pyproject.toml
  - scrapegrape/scrapegrape/settings.py
  - scrapegrape/scrapegrape/urls.py
  - scrapegrape/publishers/tasks.py
  - scrapegrape/publishers/admin.py
  - scrapegrape/publishers/views.py
  - Makefile
autonomous: true

must_haves:
  truths:
    - "docker compose up starts Redis, RQ worker, and Django services with all three healthy"
    - "An RQ job queued from Django code executes in the worker and completes successfully"
    - "RQ job queue and worker status are visible in Django admin via django-rq"
  artifacts:
    - path: "docker-compose.yml"
      provides: "Redis 7 service with healthcheck, RQ worker service, updated Django service"
      contains: "redis:7-alpine"
    - path: "scrapegrape/scrapegrape/settings.py"
      provides: "django-rq configuration replacing django_tasks"
      contains: "RQ_QUEUES"
  key_links:
    - from: "docker-compose.yml (worker service)"
      to: "scrapegrape/scrapegrape/settings.py (RQ_QUEUES)"
      via: "Worker runs manage.py rqworker which reads RQ_QUEUES pointing to redis service"
      pattern: "rqworker"
    - from: "scrapegrape/publishers/tasks.py"
      to: "django-rq @job decorator"
      via: "Replaces django_tasks @task decorator"
      pattern: "from django_rq import job"
    - from: "docker-compose.yml (django + worker)"
      to: "docker-compose.yml (redis)"
      via: "depends_on with service_healthy condition"
      pattern: "condition: service_healthy"
---

<objective>
Set up Redis, RQ worker, and django-rq infrastructure in Docker Compose, replacing the existing django_tasks backend.

Purpose: Provide a production-grade task queue foundation for the v2.0 pipeline. The existing django_tasks database backend is replaced by django-rq backed by Redis, which gives real worker processes and an admin dashboard.

Output: Updated docker-compose.yml with Redis + worker services, django-rq configured in settings, existing task code migrated from @task to @job, packages installed.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-infrastructure-models/06-RESEARCH.md
@.planning/phases/06-infrastructure-models/06-CONTEXT.md
@docker-compose.yml
@Dockerfile
@pyproject.toml
@scrapegrape/scrapegrape/settings.py
@scrapegrape/scrapegrape/urls.py
@scrapegrape/publishers/tasks.py
@scrapegrape/publishers/admin.py
@scrapegrape/publishers/views.py
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install packages and configure django-rq in Django settings</name>
  <files>
    pyproject.toml
    scrapegrape/scrapegrape/settings.py
  </files>
  <action>
    Install new dependencies:
    ```
    uv add django-rq w3lib pytest pytest-django factory-boy pytest-cov
    ```
    (w3lib, pytest, pytest-django, factory-boy, pytest-cov are installed now for Plan 02 to avoid a second install step.)

    In `pyproject.toml`:
    - Remove `django-tasks>=0.8.1` from dependencies after `uv add` completes (it will be unused).
    - Add pytest configuration:
      ```toml
      [tool.pytest.ini_options]
      DJANGO_SETTINGS_MODULE = "scrapegrape.settings"
      python_files = ["test_*.py"]
      python_classes = ["Test*"]
      python_functions = ["test_*"]
      pythonpath = ["scrapegrape"]
      ```

    In `scrapegrape/scrapegrape/settings.py`:
    - Add `import os` at the top (for environ.get).
    - Replace `"django_tasks"` and `"django_tasks.backends.database"` in INSTALLED_APPS with `"django_rq"`.
    - Remove the `TASKS = {...}` setting entirely.
    - Add RQ_QUEUES configuration:
      ```python
      RQ_QUEUES = {
          "default": {
              "HOST": os.environ.get("REDIS_HOST", "localhost"),
              "PORT": int(os.environ.get("REDIS_PORT", 6379)),
              "DB": 0,
              "DEFAULT_TIMEOUT": 600,
          },
      }
      ```
      Note: HOST defaults to "localhost" for local dev/pytest. Docker services override with REDIS_HOST=redis env var.
    - Add `PUBLISHER_FRESHNESS_TTL` setting (locked decision):
      ```python
      from datetime import timedelta
      PUBLISHER_FRESHNESS_TTL = timedelta(hours=24)
      ```

    In `scrapegrape/scrapegrape/urls.py`:
    - Add `from django.urls import include` (alongside existing `path` import).
    - Add django-rq URL pattern: `path("django-rq/", include("django_rq.urls"))` to urlpatterns. This provides the standalone queue dashboard at /django-rq/ in addition to the auto-registered admin dashboard.
  </action>
  <verify>
    Run `uv run python -c "import django_rq; print(django_rq.__version__)"` to confirm django-rq installed.
    Run `uv run python -c "import w3lib; print(w3lib.__version__)"` to confirm w3lib installed.
    Run `cd /Users/matt/src/itsascout && uv run python -c "import django; import os; os.environ['DJANGO_SETTINGS_MODULE']='scrapegrape.settings'; import sys; sys.path.insert(0,'scrapegrape'); django.setup(); from django.conf import settings; assert 'django_rq' in settings.INSTALLED_APPS; assert 'RQ_QUEUES' in dir(settings); print('Settings OK')"` to confirm Django settings load without errors.
  </verify>
  <done>django-rq, w3lib, pytest-django, factory-boy installed. Settings updated with RQ_QUEUES, django_tasks removed from INSTALLED_APPS, PUBLISHER_FRESHNESS_TTL defined, pytest config in pyproject.toml.</done>
</task>

<task type="auto">
  <name>Task 2: Add Redis and worker services to Docker Compose, migrate task code from django_tasks to django-rq</name>
  <files>
    docker-compose.yml
    Dockerfile
    scrapegrape/publishers/tasks.py
    scrapegrape/publishers/admin.py
    scrapegrape/publishers/views.py
    Makefile
  </files>
  <action>
    **Docker Compose** (`docker-compose.yml` at project root):

    Add Redis service before the django service:
    ```yaml
    redis:
      image: redis:7-alpine
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data
      healthcheck:
        test: ["CMD", "redis-cli", "ping"]
        interval: 5s
        timeout: 3s
        retries: 5
    ```

    Add worker service after the django service (same image, volumes, env as django but different command):
    ```yaml
    worker:
      build: .
      volumes:
        - .:/app
      env_file: scrapegrape/.env
      environment:
        DATABASE_URL: postgres://postgres:postgres@postgres:5432/scrapegrape
        REDIS_HOST: redis
      depends_on:
        postgres:
          condition: service_healthy
        redis:
          condition: service_healthy
      command: uv run scrapegrape/manage.py rqworker default
    ```

    Update the existing django service:
    - Add `REDIS_HOST: redis` to environment section.
    - Add redis to depends_on:
      ```yaml
      depends_on:
        postgres:
          condition: service_healthy
        redis:
          condition: service_healthy
      ```

    Add `redis_data:` to the volumes section at bottom.

    **Migrate task code:**

    In `scrapegrape/publishers/tasks.py`:
    - Replace `from django_tasks import task` with `from django_rq import job`.
    - Replace `@task` decorator on `analyze_url` with `@job("default", timeout=600)`.
    - The function body stays the same -- only the decorator and import change.

    In `scrapegrape/publishers/admin.py`:
    - Replace every `.enqueue(...)` call with `.delay(...)`. There are three locations:
      1. `queue_url_analysis` function: `analyze_url.enqueue(publisher.url)` -> `analyze_url.delay(publisher.url)`
      2. `queue_analysis_action` method: `analyze_url.enqueue(obj.url)` -> `analyze_url.delay(obj.url)`
      3. `analyze_url_view` method: `analyze_url.enqueue(url)` -> `analyze_url.delay(url)`

    In `scrapegrape/publishers/views.py`:
    - Replace `analyze_url.enqueue(publisher.url)` with `analyze_url.delay(publisher.url)` in the `create` view.
    - Replace `analyze_url.enqueue(row['URL'])` with `analyze_url.delay(row['URL'])` in the `bulk_upload` view.

    **Makefile** -- add a `worker` target:
    ```makefile
    worker: ## Tail worker logs
    	$(DC) logs -f worker
    ```
    Also add a `test` target:
    ```makefile
    test: ## Run pytest test suite
    	uv run pytest scrapegrape/ -v
    ```
  </action>
  <verify>
    Run `docker compose -f /Users/matt/src/itsascout/docker-compose.yml config` to validate docker-compose syntax.
    Grep for any remaining `.enqueue(` calls: `grep -r "\.enqueue(" scrapegrape/` should return no results.
    Grep for any remaining `django_tasks` imports: `grep -r "django_tasks" scrapegrape/` should return no results (except possibly migration files which is fine).
  </verify>
  <done>Docker Compose has Redis 7-alpine with healthcheck, RQ worker service with correct depends_on, Django service updated with REDIS_HOST. All task code migrated from django_tasks @task/.enqueue() to django-rq @job/.delay(). Makefile has worker and test targets.</done>
</task>

</tasks>

<verification>
1. `docker compose config` validates without errors (syntax check)
2. No remaining references to `django_tasks` in Python source files (except migrations)
3. No remaining `.enqueue()` calls in Python source files
4. `django_rq` is in INSTALLED_APPS, `RQ_QUEUES` is configured in settings
5. Redis service has healthcheck, worker service depends on redis with service_healthy
6. `uv run python -c "import django_rq"` succeeds
</verification>

<success_criteria>
- docker-compose.yml defines redis, worker, django, postgres, vite services
- Redis service uses redis:7-alpine with healthcheck and persistent volume
- Worker service runs `manage.py rqworker default` with same env as Django
- django-rq replaces django_tasks in INSTALLED_APPS
- RQ_QUEUES configured with REDIS_HOST env var (defaults to localhost)
- All .enqueue() calls replaced with .delay()
- All @task decorators replaced with @job
- PUBLISHER_FRESHNESS_TTL setting defined
- pytest configured in pyproject.toml
- Makefile has worker and test targets
</success_criteria>

<output>
After completion, create `.planning/phases/06-infrastructure-models/06-01-SUMMARY.md`
</output>
