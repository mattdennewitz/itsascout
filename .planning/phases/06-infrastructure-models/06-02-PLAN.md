---
phase: 06-infrastructure-models
plan: 02
type: tdd
wave: 2
depends_on: ["06-01"]
files_modified:
  - scrapegrape/publishers/models.py
  - scrapegrape/publishers/url_sanitizer.py
  - scrapegrape/publishers/factories.py
  - scrapegrape/publishers/tests/__init__.py
  - scrapegrape/publishers/tests/test_models.py
  - scrapegrape/publishers/tests/test_url_sanitizer.py
  - scrapegrape/conftest.py
  - scrapegrape/publishers/admin.py
  - scrapegrape/publishers/serializers.py
  - scrapegrape/publishers/migrations/0002_publisher_extension_resolution_job.py
autonomous: true

must_haves:
  truths:
    - "uv run pytest runs a passing test suite with factory-created Publisher and ResolutionJob instances"
    - "URL sanitizer normalizes variant URLs (http vs https, www vs bare, mixed case, query param order) to identical canonical forms"
    - "URL sanitizer strips tracking parameters (utm_*, fbclid, etc.) while preserving other query params"
    - "URL sanitizer strips fragments and preserves trailing slashes"
    - "extract_domain returns the canonical domain (www stripped) for publisher lookup"
    - "Publisher model has domain field (unique), discovery flat fields, and last_checked_at"
    - "ResolutionJob model has UUID PK, canonical_url index, status choices, and JSONField result columns"
  artifacts:
    - path: "scrapegrape/publishers/url_sanitizer.py"
      provides: "sanitize_url and extract_domain functions using w3lib"
      exports: ["sanitize_url", "extract_domain"]
    - path: "scrapegrape/publishers/models.py"
      provides: "Extended Publisher model and new ResolutionJob model"
      contains: "class ResolutionJob"
    - path: "scrapegrape/publishers/factories.py"
      provides: "PublisherFactory and ResolutionJobFactory for test data"
      exports: ["PublisherFactory", "ResolutionJobFactory"]
    - path: "scrapegrape/publishers/tests/test_url_sanitizer.py"
      provides: "Comprehensive URL sanitization edge case tests"
      min_lines: 60
    - path: "scrapegrape/publishers/tests/test_models.py"
      provides: "Model creation and relationship tests using factories"
      min_lines: 30
    - path: "scrapegrape/conftest.py"
      provides: "Shared pytest fixtures (publisher, resolution_job)"
      exports: ["publisher", "resolution_job"]
  key_links:
    - from: "scrapegrape/publishers/url_sanitizer.py"
      to: "w3lib.url"
      via: "canonicalize_url and url_query_cleaner imports"
      pattern: "from w3lib.url import"
    - from: "scrapegrape/publishers/models.py (ResolutionJob)"
      to: "scrapegrape/publishers/models.py (Publisher)"
      via: "ForeignKey relationship"
      pattern: "models.ForeignKey.*Publisher"
    - from: "scrapegrape/publishers/factories.py"
      to: "scrapegrape/publishers/models.py"
      via: "DjangoModelFactory Meta.model"
      pattern: "model = Publisher|model = ResolutionJob"
    - from: "scrapegrape/conftest.py"
      to: "scrapegrape/publishers/factories.py"
      via: "Factory imports for fixture creation"
      pattern: "from publishers.factories import"
---

<objective>
Create data models (Publisher extension, ResolutionJob), URL sanitizer with w3lib, factories, and a comprehensive pytest test suite -- all using TDD (tests first, then implementation).

Purpose: Provide the data foundation for the v2.0 pipeline. Publisher stores per-domain intelligence, ResolutionJob tracks each URL submission, and the URL sanitizer ensures consistent deduplication. Tests verify everything works and establish the project's TDD pattern.

Output: Extended Publisher model with domain/discovery fields, ResolutionJob model with UUID PK, w3lib-based URL sanitizer, factory_boy factories, pytest test suite, Django migration.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-infrastructure-models/06-RESEARCH.md
@.planning/phases/06-infrastructure-models/06-CONTEXT.md
@.planning/phases/06-infrastructure-models/06-01-SUMMARY.md
@scrapegrape/publishers/models.py
@scrapegrape/publishers/admin.py
@scrapegrape/publishers/serializers.py
@scrapegrape/publishers/forms.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for URL sanitizer, models, and factories</name>
  <files>
    scrapegrape/publishers/tests/__init__.py
    scrapegrape/publishers/tests/test_url_sanitizer.py
    scrapegrape/publishers/tests/test_models.py
    scrapegrape/conftest.py
  </files>
  <action>
    Delete the existing empty `scrapegrape/publishers/tests.py` file.
    Create `scrapegrape/publishers/tests/` directory with `__init__.py`.

    **Create `scrapegrape/conftest.py`** with shared fixtures:
    ```python
    import pytest
    from publishers.factories import PublisherFactory, ResolutionJobFactory

    @pytest.fixture
    def publisher(db):
        return PublisherFactory()

    @pytest.fixture
    def resolution_job(db):
        return ResolutionJobFactory()
    ```

    **Create `scrapegrape/publishers/tests/test_url_sanitizer.py`** with comprehensive tests:

    Test class `TestSanitizeUrl`:
    - `test_strips_www` -- "https://www.example.com/page" -> "https://example.com/page"
    - `test_bare_domain_matches_www` -- www and non-www produce identical output
    - `test_strips_fragments` -- "https://example.com/page#section" -> "https://example.com/page"
    - `test_sorts_query_params` -- "?z=1&a=2" -> "?a=2&z=1"
    - `test_strips_utm_params` -- utm_source, utm_medium removed; id=1 preserved
    - `test_strips_fbclid` -- fbclid removed; other params preserved
    - `test_strips_gclid` -- gclid removed
    - `test_lowercase_hostname` -- "EXAMPLE.COM" -> "example.com"
    - `test_preserves_trailing_slash` -- "/page/" stays "/page/"
    - `test_normalizes_http_to_https` -- "http://" -> "https://"
    - `test_preserves_non_tracking_query_params` -- q=test&page=2 preserved
    - `test_unicode_url` -- URL with unicode characters doesn't crash, contains domain
    - `test_mixed_case_scheme` -- "HTTP://" -> "https://"
    - `test_strips_all_utm_variants` -- utm_term, utm_content, utm_campaign all stripped
    - `test_empty_query_after_stripping` -- URL with only tracking params gets clean URL (no trailing ?)

    Test class `TestExtractDomain`:
    - `test_extracts_domain` -- "https://www.nytimes.com/article/123" -> "nytimes.com"
    - `test_strips_www_from_domain` -- "https://www.bbc.co.uk/news" -> "bbc.co.uk"
    - `test_extracts_domain_from_http` -- "http://example.com/page" -> "example.com"
    - `test_extracts_subdomain` -- "https://blog.example.com/post" -> "blog.example.com" (subdomain preserved, only www stripped)

    All imports: `from publishers.url_sanitizer import sanitize_url, extract_domain`

    **Create `scrapegrape/publishers/tests/test_models.py`** with model tests:

    All tests marked with `@pytest.mark.django_db`.

    Test class `TestPublisherModel`:
    - `test_factory_creates_publisher(publisher)` -- publisher.name is truthy, publisher.domain is truthy, publisher.pk is not None
    - `test_publisher_domain_unique(db)` -- create PublisherFactory(domain="example.com"), then try to create another with same domain, assert IntegrityError
    - `test_publisher_str(publisher)` -- str(publisher) returns publisher.name
    - `test_publisher_default_field_values(publisher)` -- waf_detected defaults to False, tos_url defaults to "", robots_txt_found is None, sitemap_urls defaults to [], rss_urls defaults to [], rsl_detected is None, last_checked_at is None

    Test class `TestResolutionJobModel`:
    - `test_factory_creates_job(resolution_job)` -- resolution_job.pk is not None (UUID), resolution_job.publisher is not None, resolution_job.status == "pending"
    - `test_job_uuid_pk(resolution_job)` -- isinstance(resolution_job.pk, uuid.UUID)
    - `test_job_str(resolution_job)` -- str contains "Job" and status
    - `test_job_default_results_null(resolution_job)` -- all result fields (waf_result, tos_result, robots_result, sitemap_result, rss_result, rsl_result, metadata_result) are None
    - `test_job_publisher_relationship(resolution_job)` -- resolution_job.publisher.resolution_jobs.count() == 1
    - `test_job_status_choices(db)` -- create jobs with each status ("pending", "running", "completed", "failed"), all save successfully

    Run `uv run pytest scrapegrape/ -v` -- tests MUST fail (RED). Expect ImportError or ModuleNotFoundError since url_sanitizer.py, factories.py, and updated models don't exist yet.

    Commit: `test(06-02): add failing tests for URL sanitizer, models, and factories`
  </action>
  <verify>
    `uv run pytest scrapegrape/ -v` fails with import errors (RED phase -- tests written but implementation doesn't exist yet). The test runner itself must execute (no syntax errors in test files) -- failures should be ImportError/ModuleNotFoundError, not SyntaxError.
  </verify>
  <done>Test files exist in scrapegrape/publishers/tests/ with comprehensive coverage of URL sanitization edge cases (15+ tests) and model behavior (10+ tests). conftest.py provides shared fixtures. All tests fail because implementation doesn't exist yet.</done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement models, URL sanitizer, factories, and migration</name>
  <files>
    scrapegrape/publishers/models.py
    scrapegrape/publishers/url_sanitizer.py
    scrapegrape/publishers/factories.py
    scrapegrape/publishers/admin.py
    scrapegrape/publishers/serializers.py
  </files>
  <action>
    **Create `scrapegrape/publishers/url_sanitizer.py`:**

    Use w3lib per user decision (third-party package, don't hand-roll). Research recommends w3lib canonicalize_url + url_query_cleaner.

    Define `TRACKING_PARAMS` list (utm_source, utm_medium, utm_campaign, utm_term, utm_content, fbclid, gclid, gclsrc, dclid, gbraid, wbraid, msclkid, twclid, igshid, mc_cid, mc_eid, oly_anon_id, oly_enc_id, _openstat, vero_id, wickedid, yclid, rb_clickid, s_cid, mkt_tok, trk, trkCampaign, trkInfo).

    Implement `sanitize_url(url: str) -> str`:
    1. `canonicalize_url(url, keep_fragments=False)` -- sorts query params, strips fragments, normalizes encoding
    2. `url_query_cleaner(canonical, TRACKING_PARAMS, remove=True)` -- strips tracking params (MUST use remove=True per research pitfall #6)
    3. Strip www. from hostname using urlparse/urlunparse
    4. Enforce https scheme
    5. Return cleaned URL

    Implement `extract_domain(url: str) -> str`:
    1. Call sanitize_url first
    2. Extract hostname via urlparse
    3. Return hostname

    **Extend `scrapegrape/publishers/models.py`:**

    Keep the existing Publisher class and ALL existing fields (name, url, detected_waf). Keep WAFReport class unchanged. Add new fields to Publisher:

    ```python
    # NEW: Canonical domain for publisher lookup (one domain = one publisher)
    domain = models.CharField(max_length=255, unique=True, db_index=True, default="")

    # NEW: Discovery result flat fields (populated by pipeline in later phases)
    waf_type = models.CharField(max_length=255, blank=True, default="")
    waf_detected = models.BooleanField(default=False)
    tos_url = models.URLField(blank=True, default="")
    tos_permissions = models.JSONField(null=True, blank=True)
    robots_txt_found = models.BooleanField(null=True)
    robots_txt_url_allowed = models.BooleanField(null=True)
    sitemap_urls = models.JSONField(default=list, blank=True)
    rss_urls = models.JSONField(default=list, blank=True)
    rsl_detected = models.BooleanField(null=True)

    # NEW: Freshness tracking
    last_checked_at = models.DateTimeField(null=True, blank=True)
    ```

    Add ResolutionJob model:
    ```python
    import uuid

    class ResolutionJob(models.Model):
        id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
        submitted_url = models.URLField()
        canonical_url = models.URLField(db_index=True)
        publisher = models.ForeignKey("Publisher", on_delete=models.CASCADE, related_name="resolution_jobs")
        status = models.CharField(max_length=20, default="pending", choices=[
            ("pending", "Pending"),
            ("running", "Running"),
            ("completed", "Completed"),
            ("failed", "Failed"),
        ])
        created_at = models.DateTimeField(auto_now_add=True)
        updated_at = models.DateTimeField(auto_now=True)
        # Pipeline step results stored directly on the job (locked decision)
        waf_result = models.JSONField(null=True, blank=True)
        tos_result = models.JSONField(null=True, blank=True)
        robots_result = models.JSONField(null=True, blank=True)
        sitemap_result = models.JSONField(null=True, blank=True)
        rss_result = models.JSONField(null=True, blank=True)
        rsl_result = models.JSONField(null=True, blank=True)
        metadata_result = models.JSONField(null=True, blank=True)

        class Meta:
            ordering = ["-created_at"]
            indexes = [
                models.Index(fields=["canonical_url"]),
                models.Index(fields=["status"]),
            ]

        def __str__(self):
            return f"Job {self.id} - {self.canonical_url} ({self.status})"
    ```

    **Create `scrapegrape/publishers/factories.py`:**
    ```python
    import uuid
    import factory
    from publishers.models import Publisher, ResolutionJob

    class PublisherFactory(factory.django.DjangoModelFactory):
        class Meta:
            model = Publisher
            django_get_or_create = ("domain",)

        name = factory.Sequence(lambda n: f"publisher-{n}.com")
        url = factory.LazyAttribute(lambda o: f"https://{o.name}")
        domain = factory.LazyAttribute(lambda o: o.name)

    class ResolutionJobFactory(factory.django.DjangoModelFactory):
        class Meta:
            model = ResolutionJob

        id = factory.LazyFunction(uuid.uuid4)
        submitted_url = factory.Sequence(lambda n: f"https://example-{n}.com/article")
        canonical_url = factory.LazyAttribute(lambda o: o.submitted_url)
        publisher = factory.SubFactory(PublisherFactory)
        status = "pending"
    ```

    **Create migration:**

    Run `uv run scrapegrape/manage.py makemigrations publishers` to generate migration.

    IMPORTANT (Research Pitfall #4): The `domain` field has `unique=True` but existing publishers have no domain value. The generated migration will add the field with `default=""`. Since the domain field has unique=True, if there are multiple existing publishers they'll all get domain="" and violate the constraint.

    Strategy: After makemigrations, check if there are existing rows in the publishers table. If the database is in development with no data or only test data, the simple migration is fine. If existing publishers exist, create a data migration:
    1. Add domain field as CharField(default="", unique=False) first
    2. Data migration: populate domain from url field using extract_domain logic
    3. Alter to add unique=True

    For now, since this is development: generate the migration and run it. If it fails due to existing data, split into a multi-step migration.

    Run: `uv run scrapegrape/manage.py migrate`

    **Update `scrapegrape/publishers/admin.py`:**
    - Add `"domain"` to PublisherAdmin.list_display (after "name")
    - Add `"domain"` to PublisherAdmin.search_fields
    - Register ResolutionJob in admin with basic list_display: ["id", "canonical_url", "publisher", "status", "created_at"]

    **Update `scrapegrape/publishers/serializers.py`:**
    - Add `domain` field to PublisherSerializer (if it exists) or ensure the serializer includes the new field. Review the current serializer and add any new fields that are relevant for the existing publisher table view to continue working.

    Run all tests: `uv run pytest scrapegrape/ -v`
    All tests MUST pass (GREEN).

    Commit: `feat(06-02): implement Publisher extension, ResolutionJob, URL sanitizer, and factories`
  </action>
  <verify>
    `uv run pytest scrapegrape/ -v` -- all tests pass (GREEN).
    `uv run scrapegrape/manage.py migrate --check` -- no unapplied migrations.
    `uv run scrapegrape/manage.py check --deploy 2>&1 | grep -v "WARNINGS"` -- no critical errors (warnings about SECRET_KEY etc. are expected).
  </verify>
  <done>
    Publisher model extended with domain (unique), discovery flat fields, last_checked_at. ResolutionJob model created with UUID PK, canonical_url index, status choices, 7 JSONField result columns. URL sanitizer uses w3lib canonicalize_url + url_query_cleaner with tracking param denylist. Factories create valid test data. All tests pass. Migration applied. Admin updated with ResolutionJob. Existing publisher table and admin continue working.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest scrapegrape/ -v` -- all tests pass
2. URL sanitizer normalizes: http->https, strips www, strips fragments, sorts query params, strips tracking params, preserves trailing slashes
3. `sanitize_url("https://www.example.com/page?utm_source=fb&id=1#section")` == `sanitize_url("http://EXAMPLE.COM/page?id=1")`
4. Publisher model has: domain (unique), waf_type, waf_detected, tos_url, tos_permissions, robots_txt_found, robots_txt_url_allowed, sitemap_urls, rss_urls, rsl_detected, last_checked_at
5. ResolutionJob model has: UUID PK, submitted_url, canonical_url (indexed), publisher FK, status, created_at, updated_at, 7 result JSONFields
6. Factories create valid instances without errors
7. No unapplied migrations
8. Existing publisher admin and table view still work (no removed fields)
</verification>

<success_criteria>
- All pytest tests pass (15+ URL sanitizer tests, 10+ model tests)
- Publisher model extended in-place with all discovery fields as flat fields (locked decision)
- ResolutionJob model has UUID PK and all pipeline result JSONFields (locked decision)
- URL sanitizer uses w3lib (not hand-rolled, locked decision)
- URL sanitizer strips tracking params, www, fragments; preserves trailing slashes (locked decisions)
- factory_boy factories for Publisher and ResolutionJob
- conftest.py with shared fixtures
- Django migration applied successfully
- Existing admin and views unbroken
</success_criteria>

<output>
After completion, create `.planning/phases/06-infrastructure-models/06-02-SUMMARY.md`
</output>
