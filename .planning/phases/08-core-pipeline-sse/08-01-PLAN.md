---
phase: 08-core-pipeline-sse
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - scrapegrape/publishers/pipeline/__init__.py
  - scrapegrape/publishers/pipeline/events.py
  - scrapegrape/publishers/pipeline/steps.py
  - scrapegrape/publishers/pipeline/supervisor.py
  - scrapegrape/publishers/tests/test_pipeline.py
  - pyproject.toml
  - uv.lock
autonomous: true

must_haves:
  truths:
    - "Pipeline supervisor runs all steps sequentially and publishes Redis events between steps"
    - "WAF step calls wafw00f and returns structured result with waf_detected and waf_type"
    - "ToS discovery step uses FetchStrategyManager and terms discovery agent to find ToS URL"
    - "ToS evaluation step fetches and evaluates ToS permissions"
    - "Pipeline skips publisher-level steps when publisher was checked within freshness TTL"
    - "Each pipeline step saves its result to the ResolutionJob before publishing the event"
    - "Pipeline sets job status to failed on unhandled exception"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/supervisor.py"
      provides: "Pipeline supervisor RQ job"
      contains: "run_pipeline"
    - path: "scrapegrape/publishers/pipeline/steps.py"
      provides: "Individual pipeline step functions"
      contains: "run_waf_step"
    - path: "scrapegrape/publishers/pipeline/events.py"
      provides: "Redis pub/sub event publishing"
      contains: "publish_step_event"
    - path: "scrapegrape/publishers/tests/test_pipeline.py"
      provides: "Pipeline unit tests with mocked externals"
  key_links:
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/steps.py"
      via: "function call"
      pattern: "run_waf_step|run_tos_discovery_step|run_tos_evaluation_step"
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/events.py"
      via: "publish_step_event after each step"
      pattern: "publish_step_event"
    - from: "scrapegrape/publishers/pipeline/steps.py"
      to: "scrapegrape/publishers/fetchers/manager.py"
      via: "FetchStrategyManager.fetch for ToS page fetching"
      pattern: "FetchStrategyManager"
---

<objective>
Create the pipeline supervisor RQ job, individual step functions (WAF, ToS discovery, ToS evaluation), Redis event publishing helper, and freshness TTL check -- all with TDD.

Purpose: The pipeline is the core backend engine that runs when a user submits a URL. Each step must save results to the ResolutionJob and publish events to Redis pub/sub so the SSE endpoint (Plan 02) can stream progress to the frontend (Plan 03).

Output: Working pipeline module with supervisor, 3 step functions, event publisher, and comprehensive test suite.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-core-pipeline-sse/08-RESEARCH.md
@.planning/phases/06-infrastructure-models/06-02-SUMMARY.md
@.planning/phases/07-fetch-strategy/07-01-SUMMARY.md

# Key existing code
@scrapegrape/publishers/models.py
@scrapegrape/publishers/tasks.py
@scrapegrape/publishers/waf_check.py
@scrapegrape/publishers/fetchers/manager.py
@scrapegrape/publishers/fetchers/base.py
@scrapegrape/publishers/factories.py
@scrapegrape/conftest.py
@scrapegrape/scrapegrape/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for pipeline steps, supervisor, and events</name>
  <files>
    scrapegrape/publishers/tests/test_pipeline.py
  </files>
  <action>
Create `scrapegrape/publishers/tests/test_pipeline.py` with tests for all pipeline components. Install `pytest-asyncio` via `uv add --dev pytest-asyncio`. All tests should use monkeypatch to mock external services. Use existing `PublisherFactory` and `ResolutionJobFactory` from `publishers.factories`.

**Test class: TestPublishStepEvent**
- `test_publish_step_event_sends_to_redis_channel`: Mock `redis.Redis` and verify `publish_step_event(job_id, "waf", "completed", {"waf_detected": True})` calls `r.publish(f"job:{job_id}:events", json_payload)` with correct channel and JSON data.

**Test class: TestShouldSkipPublisherSteps**
- `test_skip_when_recently_checked`: Create publisher with `last_checked_at = timezone.now() - timedelta(hours=1)`. Assert `should_skip_publisher_steps(publisher)` returns `True` (within 24h TTL).
- `test_no_skip_when_stale`: Create publisher with `last_checked_at = timezone.now() - timedelta(hours=25)`. Assert returns `False`.
- `test_no_skip_when_never_checked`: Create publisher with `last_checked_at = None`. Assert returns `False`.

**Test class: TestRunWafStep**
- `test_waf_step_detected`: Monkeypatch `publishers.pipeline.steps.scan_url_with_wafw00f` to return `{"report": [{"detected": True, "firewall": "Cloudflare", "manufacturer": "Cloudflare Inc.", "url": "https://example.com"}]}`. Call `run_waf_step(publisher)`. Assert result has `waf_detected=True`, `waf_type="Cloudflare"`.
- `test_waf_step_not_detected`: Monkeypatch to return `{"report": [{"detected": False, "firewall": "None", "manufacturer": "", "url": "..."}]}`. Assert `waf_detected=False`.
- `test_waf_step_scan_failure`: Monkeypatch to return `None`. Assert result has `waf_detected=False` and `error` key.

**Test class: TestRunTosDiscoveryStep**
- `test_tos_discovery_finds_url`: Monkeypatch `publishers.pipeline.steps.discover_terms_and_privacy` to return a mock result object with `terms_of_service_url="https://example.com/tos"`, `confidence_score=0.9`, `notes="Found via link"`. Assert result has `tos_url` and `confidence`.
- `test_tos_discovery_no_url_found`: Monkeypatch to return mock with `terms_of_service_url=None`. Assert result has `tos_url=None`.

**Test class: TestRunTosEvaluationStep**
- `test_tos_evaluation_returns_permissions`: Monkeypatch `publishers.pipeline.steps.evaluate_terms_and_conditions` to return a mock with `permissions=[MockPermission(activity="scraping", permission="allowed")]`, `confidence_score=0.85`, `document_type="Terms of Service"`. Assert result has `permissions` list and `document_type`.
- `test_tos_evaluation_no_tos_url`: Call with `tos_url=None`. Assert result indicates skip/no evaluation.

**Test class: TestRunPipeline**
- `test_pipeline_runs_all_steps`: Monkeypatch all step functions (`run_waf_step`, `run_tos_discovery_step`, `run_tos_evaluation_step`) and `publish_step_event`. Create a ResolutionJob. Call `run_pipeline(str(job.id))`. Assert job.status becomes "completed", all step functions were called, events were published for each step including terminal "pipeline completed" event.
- `test_pipeline_skips_fresh_publisher`: Set publisher `last_checked_at` to 1 hour ago. Monkeypatch step functions. Call `run_pipeline`. Assert step functions NOT called. Assert skip events published.
- `test_pipeline_sets_failed_on_exception`: Monkeypatch `run_waf_step` to raise `Exception("network error")`. Call `run_pipeline`. Assert job.status becomes "failed". Assert pipeline failed event published.
- `test_pipeline_saves_step_results`: Monkeypatch steps to return known data. Call `run_pipeline`. Reload job from DB. Assert `job.waf_result`, `job.tos_result` contain the mocked data.

Run tests: `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v`. All tests MUST FAIL (RED phase -- modules don't exist yet). Commit with message: `test(phase-08): add failing tests for pipeline supervisor and step functions`.
  </action>
  <verify>`uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v 2>&1 | grep -E "(FAILED|ERROR)" | wc -l` should show all tests failing (import errors or assertion errors)</verify>
  <done>Test file exists with 14+ tests covering event publishing, freshness TTL, WAF step, ToS discovery step, ToS evaluation step, and pipeline supervisor (full run, skip, fail, result persistence). All tests fail because implementation doesn't exist yet.</done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement pipeline module (events, steps, supervisor)</name>
  <files>
    scrapegrape/publishers/pipeline/__init__.py
    scrapegrape/publishers/pipeline/events.py
    scrapegrape/publishers/pipeline/steps.py
    scrapegrape/publishers/pipeline/supervisor.py
  </files>
  <action>
Create the `scrapegrape/publishers/pipeline/` package with four files:

**`__init__.py`:**
Export `run_pipeline` from supervisor and `publish_step_event` from events.

**`events.py`:**
- `get_redis_client()`: Create synchronous `redis.Redis` using `os.environ.get("REDIS_HOST", "localhost")` and `REDIS_PORT` (matching existing RQ_QUEUES pattern in settings).
- `publish_step_event(job_id: str, step: str, status: str, data: dict | None = None)`: Create Redis client, JSON-encode `{"step": step, "status": status, "data": data or {}}`, publish to channel `job:{job_id}:events`.

**`steps.py`:**
- `should_skip_publisher_steps(publisher) -> bool`: Compare `publisher.last_checked_at` against `settings.PUBLISHER_FRESHNESS_TTL`. Return `True` if age < TTL. Return `False` if `last_checked_at` is None.
- `run_waf_step(publisher) -> dict`: Import `scan_url_with_wafw00f` from `publishers.waf_check`. Call with `publisher.url`. Parse result: if scan returns valid data, extract `detected` and `firewall` from `result["report"][0]`. Return `{"waf_detected": bool, "waf_type": str}`. On failure (None returned), return `{"waf_detected": False, "waf_type": "", "error": "WAF scan failed"}`.
- `run_tos_discovery_step(publisher) -> dict`: Import `discover_terms_and_privacy` from `ingestion.terms_discovery`. Call with `publisher.url`. Extract `terms_of_service_url`, `confidence_score`, `notes`. Return `{"tos_url": str|None, "confidence": float, "notes": str}`. On exception, return `{"tos_url": None, "error": str(e)}`.
- `run_tos_evaluation_step(publisher, tos_url: str | None) -> dict`: If `tos_url` is None, return `{"skipped": True, "reason": "No ToS URL found"}`. Otherwise, import `evaluate_terms_and_conditions` from `ingestion.terms_evaluation`. Call with `tos_url`. Extract permissions, document_type, confidence_score. Return structured dict. On exception, return `{"error": str(e)}`.

**`supervisor.py`:**
- `run_pipeline(job_id: str)`: Decorated with `@job("default", timeout=600)`.
  1. Load `ResolutionJob` by id (select_related publisher).
  2. Set `status = "running"`, save.
  3. Publish `("publisher_resolution", "completed", {publisher_name, domain})`.
  4. Check `should_skip_publisher_steps(publisher)`:
     - If skip: publish skip events for waf, tos_discovery, tos_evaluation.
     - If not skip:
       a. Publish `("waf", "started")`. Call `run_waf_step`. Save `waf_result` on job. Publish `("waf", "completed", result)`. Also update publisher flat fields: `waf_detected`, `waf_type`.
       b. Publish `("tos_discovery", "started")`. Call `run_tos_discovery_step`. Save `tos_result` on job. Publish `("tos_discovery", "completed", result)`. Also update publisher: `tos_url`.
       c. Publish `("tos_evaluation", "started")`. Call `run_tos_evaluation_step(publisher, tos_url)`. Update `tos_result` with evaluation data (merge into existing tos_result dict). Publish `("tos_evaluation", "completed", result)`. Also update publisher: `tos_permissions`.
       d. Update `publisher.last_checked_at = timezone.now()`. Save publisher.
  5. Set `status = "completed"`. Save job.
  6. Publish `("pipeline", "completed")`.
  7. Wrap entire body in try/except: on exception, set `status = "failed"`, save job, publish `("pipeline", "failed", {"error": str(e)})`, re-raise.

Run full test suite: `uv run pytest scrapegrape/ -v`. All tests MUST PASS. Commit with message: `feat(phase-08): implement pipeline supervisor with WAF and ToS steps`.
  </action>
  <verify>`uv run pytest scrapegrape/ -v` -- all tests pass (existing 45 + new 14+ pipeline tests). Zero failures.</verify>
  <done>Pipeline module exists with supervisor, 3 step functions, event publisher, and freshness TTL check. All tests pass. Supervisor saves results to ResolutionJob and publishes Redis events. Step functions mock-tested with all external services mocked.</done>
</task>

</tasks>

<verification>
1. `uv run pytest scrapegrape/ -v` -- all tests pass (existing + new pipeline tests)
2. `python -c "from publishers.pipeline import run_pipeline, publish_step_event"` -- imports succeed
3. `python -c "from publishers.pipeline.steps import run_waf_step, run_tos_discovery_step, run_tos_evaluation_step, should_skip_publisher_steps"` -- imports succeed
4. Verify test count increased by 14+ from baseline of 45
</verification>

<success_criteria>
- Pipeline supervisor can be called with a ResolutionJob UUID and executes steps sequentially
- Each step saves results to the ResolutionJob model
- Redis events are published after each step completion
- Freshness TTL check correctly skips publisher-level steps
- All external services (wafw00f, terms discovery, terms evaluation) are mocked in tests
- Existing 45 tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-core-pipeline-sse/08-01-SUMMARY.md`
</output>
