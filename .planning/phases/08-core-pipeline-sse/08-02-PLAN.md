---
phase: 08-core-pipeline-sse
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - scrapegrape/scrapegrape/settings.py
  - scrapegrape/scrapegrape/urls.py
  - scrapegrape/scrapegrape/asgi.py
  - scrapegrape/publishers/views.py
  - scrapegrape/publishers/tests/test_views.py
  - pyproject.toml
  - uv.lock
autonomous: true

must_haves:
  truths:
    - "POST /submit with a URL creates a ResolutionJob and redirects to /jobs/<uuid>"
    - "POST /submit with a duplicate completed URL redirects to the existing job"
    - "GET /jobs/<uuid> renders the Jobs/Show Inertia page with job data"
    - "GET /api/jobs/<uuid>/stream returns text/event-stream content type"
    - "Daphne serves as ASGI server (manage.py runserver shows Daphne startup)"
    - "Existing publisher table at / continues working unchanged"
  artifacts:
    - path: "scrapegrape/scrapegrape/settings.py"
      provides: "Daphne in INSTALLED_APPS and ASGI_APPLICATION"
      contains: "daphne"
    - path: "scrapegrape/scrapegrape/urls.py"
      provides: "Routes for submit, job show, and SSE stream"
      contains: "job-stream"
    - path: "scrapegrape/publishers/views.py"
      provides: "submit_url, job_show, job_stream views"
      contains: "job_stream"
    - path: "scrapegrape/publishers/tests/test_views.py"
      provides: "Tests for URL submission and job views"
  key_links:
    - from: "scrapegrape/publishers/views.py (submit_url)"
      to: "scrapegrape/publishers/pipeline/supervisor.py"
      via: "run_pipeline.delay(str(job.id))"
      pattern: "run_pipeline\\.delay"
    - from: "scrapegrape/publishers/views.py (job_stream)"
      to: "Redis pub/sub"
      via: "redis.asyncio subscribe to job:{uuid}:events"
      pattern: "pubsub\\.subscribe"
    - from: "scrapegrape/scrapegrape/urls.py"
      to: "scrapegrape/publishers/views.py"
      via: "URL routing"
      pattern: "submit_url|job_show|job_stream"
---

<objective>
Add Daphne ASGI server configuration, URL submission endpoint with deduplication, SSE stream endpoint with Redis pub/sub subscription, and Inertia job show view -- completing the backend endpoint layer for the core pipeline.

Purpose: Users need to submit a URL (POST /submit), see results at a shareable URL (/jobs/<uuid>), and receive real-time SSE events (/api/jobs/<uuid>/stream). Daphne enables async SSE views without tying worker threads.

Output: Working backend endpoints with Daphne ASGI, URL submission, SSE streaming, and job page rendering.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-core-pipeline-sse/08-RESEARCH.md
@.planning/phases/08-core-pipeline-sse/08-01-SUMMARY.md
@.planning/phases/06-infrastructure-models/06-02-SUMMARY.md

# Key existing code
@scrapegrape/publishers/models.py
@scrapegrape/publishers/views.py
@scrapegrape/publishers/url_sanitizer.py
@scrapegrape/publishers/factories.py
@scrapegrape/scrapegrape/settings.py
@scrapegrape/scrapegrape/urls.py
@scrapegrape/scrapegrape/asgi.py
@scrapegrape/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configure Daphne ASGI and add URL submission + job views</name>
  <files>
    scrapegrape/scrapegrape/settings.py
    scrapegrape/scrapegrape/urls.py
    scrapegrape/scrapegrape/asgi.py
    scrapegrape/publishers/views.py
    pyproject.toml
    uv.lock
  </files>
  <action>
**Step 1: Install Daphne**
Run `uv add daphne`. This adds the Daphne ASGI server.

**Step 2: Configure Daphne in Django settings**
Edit `scrapegrape/scrapegrape/settings.py`:
- Add `"daphne"` as the FIRST entry in `INSTALLED_APPS` (before `django.contrib.admin`). Daphne must be first to hook into `manage.py runserver`.
- Add `ASGI_APPLICATION = "scrapegrape.asgi.application"` after the existing `WSGI_APPLICATION` line.

**Step 3: Verify asgi.py is correct**
The existing `scrapegrape/scrapegrape/asgi.py` already has `get_asgi_application()`. No changes needed unless something is missing.

**Step 4: Add views to `scrapegrape/publishers/views.py`**
Add three new view functions BELOW the existing views (do NOT modify existing `table`, `create`, `update`, `bulk_upload` views):

`submit_url(request)`:
- Only handle POST. If GET, redirect to "/".
- Get `url` from `request.POST.get("url", "").strip()`.
- If empty, set session error flash and redirect to "/".
- Call `sanitize_url(url)` from `publishers.url_sanitizer` to get canonical URL.
- Call `extract_domain(url)` to get domain.
- Check for existing completed job: `ResolutionJob.objects.filter(canonical_url=canonical_url, status="completed").first()`. If found, redirect to `/jobs/{existing.id}`.
- Get or create publisher: `Publisher.objects.get_or_create(domain=domain, defaults={"name": domain, "url": canonical_url})`.
- Create new job: `ResolutionJob.objects.create(submitted_url=url, canonical_url=canonical_url, publisher=publisher)`.
- Queue pipeline: `from publishers.pipeline import run_pipeline; run_pipeline.delay(str(job.id))`.
- Redirect to `/jobs/{job.id}`.

`job_show(request, job_id)`:
- Load `ResolutionJob.objects.select_related("publisher").get(id=job_id)`. Use `get_object_or_404` pattern (catch DoesNotExist, return 404).
- Return `inertia_render(request, "Jobs/Show", props={...})` with job data:
  - `id`: str(job.id)
  - `status`: job.status
  - `canonical_url`: job.canonical_url
  - `submitted_url`: job.submitted_url
  - `publisher_name`: job.publisher.name
  - `publisher_domain`: job.publisher.domain
  - `waf_result`: job.waf_result
  - `tos_result`: job.tos_result
  - `created_at`: job.created_at.isoformat()

`job_stream(request, job_id)`:
- This is an `async` view function.
- First, verify the job exists: `await ResolutionJob.objects.filter(id=job_id).aexists()`. If not, return `HttpResponseNotFound()`.
- Check if job is already completed/failed: load status via `await ResolutionJob.objects.filter(id=job_id).values_list("status", flat=True).afirst()`. If status is "completed" or "failed", yield a single event with the current job state from DB and close.
- Define an async generator `event_generator()`:
  - Import `redis.asyncio as aioredis`.
  - Create connection: `aioredis.Redis(host=settings.RQ_QUEUES["default"]["HOST"], port=settings.RQ_QUEUES["default"]["PORT"])`.
  - Subscribe to `job:{job_id}:events`.
  - `async for message in pubsub.listen()`: skip non-message types. Decode bytes to string. Yield `f"data: {data}\n\n"`. Parse JSON -- if `step == "pipeline"` and `status in ("completed", "failed")`, yield `f"event: done\ndata: {data}\n\n"` and break.
  - In `finally`: unsubscribe, close pubsub, close redis.
- Return `StreamingHttpResponse(streaming_content=event_generator(), content_type="text/event-stream")` with headers `Cache-Control: no-cache` and `X-Accel-Buffering: no`.

**Step 5: Add URL routes**
Edit `scrapegrape/scrapegrape/urls.py`. Add three new paths BEFORE the catch-all `""` route:
```python
path("submit", publishers.views.submit_url, name="submit-url"),
path("jobs/<uuid:job_id>", publishers.views.job_show, name="job-show"),
path("api/jobs/<uuid:job_id>/stream", publishers.views.job_stream, name="job-stream"),
```

The catch-all `""` route for the publisher table MUST remain last.
  </action>
  <verify>
- `uv run python -c "import daphne; print(daphne.__version__)"` -- Daphne installed
- `uv run python manage.py check` (from scrapegrape/) -- no errors
- `uv run pytest scrapegrape/ -v` -- all existing tests still pass
  </verify>
  <done>Daphne configured as ASGI server. Three new view functions (submit_url, job_show, job_stream) added. Three new URL routes registered. Existing views and routes untouched.</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for URL submission, deduplication, and job views</name>
  <files>
    scrapegrape/publishers/tests/test_views.py
  </files>
  <action>
Create `scrapegrape/publishers/tests/test_views.py` with tests for the new endpoints. Use the Django test client, existing factories, and monkeypatch.

**Test class: TestSubmitUrl**
- `test_submit_url_creates_job_and_redirects(client, db, monkeypatch)`: Monkeypatch `publishers.views.run_pipeline` with a mock (so no actual RQ job is queued). POST to `/submit` with `{"url": "https://example.com/article"}`. Assert response is 302 redirect. Assert redirect URL contains `/jobs/`. Assert a ResolutionJob was created in DB. Assert `run_pipeline.delay` was called with the job's UUID string.
- `test_submit_duplicate_url_redirects_to_existing(client, db, monkeypatch)`: Create a completed ResolutionJob via factory with `canonical_url="https://example.com/article"`, `status="completed"`. Monkeypatch `run_pipeline`. POST to `/submit` with `{"url": "https://example.com/article"}`. Assert redirect URL contains the existing job's UUID. Assert no new ResolutionJob was created.
- `test_submit_empty_url_redirects_with_error(client, db)`: POST to `/submit` with `{"url": ""}`. Assert redirect to `/`. Assert session contains error flash.
- `test_submit_url_get_redirects_to_home(client)`: GET `/submit`. Assert redirect to `/`.
- `test_submit_url_creates_publisher_from_domain(client, db, monkeypatch)`: Monkeypatch `run_pipeline`. POST with `{"url": "https://www.example.com/page"}`. Assert Publisher was created with `domain="example.com"`.

**Test class: TestJobShow**
- `test_job_show_returns_200(client, db)`: Create a ResolutionJob via factory. GET `/jobs/{job.id}`. Assert 200 status (Inertia will return 200 with page component).
- `test_job_show_404_for_nonexistent(client, db)`: GET `/jobs/{random_uuid}`. Assert 404.

**Test class: TestJobStream**
- `test_job_stream_returns_event_stream_content_type(client, db)`: Create a completed ResolutionJob. GET `/api/jobs/{job.id}/stream`. Assert `Content-Type` is `text/event-stream`. (For a completed job, the view should return the terminal state and close.)
- `test_job_stream_404_for_nonexistent(client, db)`: GET `/api/jobs/{random_uuid}/stream`. Assert 404.

**Test class: TestExistingViewsUnchanged**
- `test_publisher_table_still_works(client, db)`: GET `/`. Assert 200.
- `test_publisher_create_still_works(client, db)`: GET `/publishers/create`. Assert 200.

Run: `uv run pytest scrapegrape/ -v`. All tests pass. Commit with message: `feat(phase-08): add Daphne ASGI, URL submission, SSE endpoint, and job views`.
  </action>
  <verify>`uv run pytest scrapegrape/ -v` -- all tests pass including new view tests. Test count should be ~70+ (45 existing + 14 pipeline + 11 view tests).</verify>
  <done>URL submission creates jobs and redirects. Duplicate URLs redirect to existing completed jobs. Job show renders Inertia page. SSE endpoint returns text/event-stream. All existing views continue working. Full test coverage for new endpoints.</done>
</task>

</tasks>

<verification>
1. `uv run pytest scrapegrape/ -v` -- all tests pass
2. `uv run python manage.py check` -- no Django errors
3. Verify Daphne is first in INSTALLED_APPS and ASGI_APPLICATION is set
4. Existing publisher table at `/` returns 200
5. URL routes include `/submit`, `/jobs/<uuid>`, `/api/jobs/<uuid>/stream`
</verification>

<success_criteria>
- Daphne configured as ASGI server for async SSE support
- URL submission endpoint deduplicates against completed jobs
- SSE endpoint streams Redis pub/sub events with correct content type
- Job show page renders via Inertia with full job data
- All existing views and admin continue working unchanged
- Test suite covers URL submission, deduplication, job show, SSE endpoint, and backward compatibility
</success_criteria>

<output>
After completion, create `.planning/phases/08-core-pipeline-sse/08-02-SUMMARY.md`
</output>
