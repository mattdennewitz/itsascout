---
phase: 09-publisher-discovery
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - scrapegrape/publishers/pipeline/steps.py
  - scrapegrape/publishers/pipeline/supervisor.py
  - scrapegrape/publishers/tests/test_pipeline.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "robots.txt step fetches /robots.txt, parses with protego, and reports url_allowed for the submitted URL"
    - "robots.txt step extracts Sitemap directives and License directives from robots.txt"
    - "robots.txt step handles missing/malformed robots.txt gracefully (returns robots_found=false)"
    - "robots.txt step detects HTML error pages masquerading as robots.txt (content-type check)"
    - "sitemap step combines sitemaps from robots.txt with common-path probing when robots.txt has none"
    - "sitemap step stores discovered sitemap URLs on the publisher"
    - "supervisor runs robots and sitemap steps after ToS evaluation, saves results to ResolutionJob, publishes SSE events, updates publisher flat fields"
    - "supervisor publishes skip events for robots/sitemap when publisher is fresh"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/steps.py"
      provides: "run_robots_step and run_sitemap_step functions"
      contains: "run_robots_step"
    - path: "scrapegrape/publishers/pipeline/supervisor.py"
      provides: "robots and sitemap steps integrated into pipeline"
      contains: "run_robots_step"
    - path: "scrapegrape/publishers/tests/test_pipeline.py"
      provides: "Tests for robots and sitemap steps"
      contains: "TestRunRobotsStep"
  key_links:
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/steps.py"
      via: "import and call run_robots_step, run_sitemap_step"
      pattern: "from publishers.pipeline.steps import.*run_robots_step"
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/models.py"
      via: "resolution_job.robots_result, resolution_job.sitemap_result saves"
      pattern: "resolution_job\\.robots_result"
---

<objective>
Add robots.txt fetching/parsing and sitemap discovery as pipeline steps with full test coverage.

Purpose: Enables the pipeline to report whether a submitted URL is allowed by robots.txt and to discover sitemap locations -- two core publisher crawling policy signals.

Output: Two new step functions (run_robots_step, run_sitemap_step) in steps.py, supervisor integration, and tests.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-publisher-discovery/09-RESEARCH.md
@scrapegrape/publishers/pipeline/steps.py
@scrapegrape/publishers/pipeline/supervisor.py
@scrapegrape/publishers/pipeline/events.py
@scrapegrape/publishers/models.py
@scrapegrape/publishers/tests/test_pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add protego dependency and implement robots.txt + sitemap step functions with tests</name>
  <files>
    pyproject.toml
    scrapegrape/publishers/pipeline/steps.py
    scrapegrape/publishers/tests/test_pipeline.py
  </files>
  <action>
1. Install protego: `uv add protego`

2. In `steps.py`, add two new step functions following the exact pattern of existing steps (return plain dicts, catch exceptions, log errors):

**`run_robots_step(publisher, submitted_url: str) -> dict`:**
- Build robots URL: `urljoin(f"https://{publisher.domain}/", "/robots.txt")`
- Fetch with `requests.get(robots_url, timeout=15, headers={"User-Agent": "itsascout"})`
- Content-type guard: if response is 200 but Content-Type contains "text/html", treat as "not found" (WAF HTML challenge page -- see Pitfall 1 in research)
- Parse with `Protego.parse(response.text)` wrapped in try/except
- Check `rp.can_fetch(submitted_url, "itsascout")` for URL allowance
- Extract sitemaps via `list(rp.sitemaps)`
- Extract crawl delay via `rp.crawl_delay("itsascout")`
- Extract License directives with `_extract_license_directives(response.text)` using regex `r'^License:\s*(.+)$'` with MULTILINE|IGNORECASE
- Return dict: `{"robots_found": True, "url_allowed": bool, "sitemaps_from_robots": list, "crawl_delay": float|None, "license_directives": list, "raw_length": int}`
- On non-200 status: return `{"robots_found": False, "status_code": int}`
- On RequestException: return `{"robots_found": False, "error": str}`
- On Protego parse error: return `{"robots_found": False, "error": "malformed robots.txt"}`

**`run_sitemap_step(publisher, robots_result: dict) -> dict`:**
- Start with sitemaps found in robots_result: `set(robots_result.get("sitemaps_from_robots", []))`
- Resolve any relative sitemap URLs with `urljoin(f"https://{publisher.domain}/", url)`
- If no sitemaps from robots.txt, probe common paths in order: `/sitemap.xml`, `/sitemap_index.xml`, `/sitemap/sitemap.xml`, `/wp-sitemap.xml`
- Probe with `requests.head(url, timeout=10, allow_redirects=True)` -- if 200 and content-type contains "xml", add to found set and break
- If HEAD returns 405 or ConnectionError, fall back to `requests.get(url, timeout=10, stream=True)` then close immediately -- check status and content-type headers (see Pitfall 7 in research)
- Return: `{"sitemap_urls": sorted(list), "source": "robots.txt"|"probe"|"none", "count": int}`

**Helper `_extract_license_directives(robots_text: str) -> list[str]`:**
- Regex extraction of License: directive values from raw robots.txt text
- Returns list of URL strings

3. In `test_pipeline.py`, add TDD-style test classes:

**`TestRunRobotsStep`:**
- `test_robots_found_url_allowed`: Mock requests.get to return 200 text/plain with standard robots.txt. Assert robots_found=True, url_allowed=True, sitemaps extracted.
- `test_robots_found_url_disallowed`: Mock with robots.txt that disallows the submitted path. Assert url_allowed=False.
- `test_robots_not_found_404`: Mock 404 response. Assert robots_found=False.
- `test_robots_html_challenge_page`: Mock 200 with content-type text/html. Assert robots_found=False (WAF interception).
- `test_robots_network_error`: Mock requests.get to raise ConnectionError. Assert robots_found=False, error key present.
- `test_robots_malformed_content`: Mock 200 text/plain with binary garbage. Wrap Protego.parse in try/except. Assert robots_found=False.
- `test_robots_extracts_license_directives`: Mock robots.txt with `License: https://example.com/license.xml`. Assert license_directives contains the URL.
- `test_robots_extracts_crawl_delay`: Mock robots.txt with `Crawl-delay: 5`. Assert crawl_delay=5.0.

**`TestExtractLicenseDirectives`:**
- `test_single_license`: One License line -> list of one URL.
- `test_multiple_licenses`: Two License lines -> list of two URLs.
- `test_no_license`: No License lines -> empty list.
- `test_case_insensitive`: "license:" lowercase -> still extracted.

**`TestRunSitemapStep`:**
- `test_sitemaps_from_robots`: robots_result has sitemaps_from_robots. Assert those URLs returned, source="robots.txt".
- `test_sitemaps_from_probe`: robots_result has no sitemaps. Mock HEAD returning 200 with xml content-type for /sitemap.xml. Assert found, source="probe".
- `test_no_sitemaps_found`: robots_result empty, all probes return 404. Assert count=0, source="none".
- `test_probe_stops_at_first_success`: Mock first probe succeeds. Assert only one sitemap found (not all probed).
- `test_relative_sitemap_urls_resolved`: robots_result has "/sitemap.xml" (relative). Assert resolved to full URL.

Use monkeypatch on `publishers.pipeline.steps.requests.get` and `publishers.pipeline.steps.requests.head` for mocking. Use PublisherFactory for publisher instances.
  </action>
  <verify>
Run `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v` -- all existing tests pass, all new tests pass. Verify protego is in pyproject.toml dependencies.
  </verify>
  <done>
run_robots_step and run_sitemap_step return correct dicts for all cases (found, not found, error, WAF challenge). All new tests pass. protego installed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate robots and sitemap steps into pipeline supervisor</name>
  <files>
    scrapegrape/publishers/pipeline/supervisor.py
    scrapegrape/publishers/tests/test_pipeline.py
  </files>
  <action>
1. In `supervisor.py`, import the new step functions:
```python
from publishers.pipeline.steps import (
    run_robots_step,
    run_sitemap_step,
    run_tos_discovery_step,
    run_tos_evaluation_step,
    run_waf_step,
    should_skip_publisher_steps,
)
```

2. Add robots and sitemap steps AFTER the ToS evaluation block (Step 3) but BEFORE the freshness timestamp update. Follow the exact same pattern as existing steps:

**Step 4: robots.txt + URL allowance:**
```python
# Step 4: robots.txt
publish_step_event(job_id, "robots", "started")
robots_result = run_robots_step(publisher, resolution_job.canonical_url)
resolution_job.robots_result = robots_result
resolution_job.save(update_fields=["robots_result"])
publish_step_event(job_id, "robots", "completed", robots_result)

# Update publisher flat fields
publisher.robots_txt_found = robots_result.get("robots_found", False)
publisher.robots_txt_url_allowed = robots_result.get("url_allowed")
publisher.save(update_fields=["robots_txt_found", "robots_txt_url_allowed"])
```

**Step 5: Sitemap discovery:**
```python
# Step 5: Sitemap discovery
publish_step_event(job_id, "sitemap", "started")
sitemap_result = run_sitemap_step(publisher, robots_result)
resolution_job.sitemap_result = sitemap_result
resolution_job.save(update_fields=["sitemap_result"])
publish_step_event(job_id, "sitemap", "completed", sitemap_result)

publisher.sitemap_urls = sitemap_result.get("sitemap_urls", [])
publisher.save(update_fields=["sitemap_urls"])
```

3. In the freshness skip branch (the `if should_skip_publisher_steps` block), add skip events for the new steps:
```python
publish_step_event(job_id, "robots", "skipped", {"reason": "fresh"})
publish_step_event(job_id, "sitemap", "skipped", {"reason": "fresh"})
```

4. Update existing tests in `TestRunPipeline`:

- **`test_pipeline_runs_all_steps`**: Add monkeypatches for run_robots_step and run_sitemap_step. Assert "robots" and "sitemap" appear in step_names.
- **`test_pipeline_skips_fresh_publisher`**: Add monkeypatches for new steps. Assert ("robots", "skipped") and ("sitemap", "skipped") in events_published.
- **`test_pipeline_saves_step_results`**: Add monkeypatches for new steps. After run, assert job.robots_result and job.sitemap_result are saved.

5. Add new test:
- **`test_pipeline_updates_publisher_robots_and_sitemap_fields`**: Run pipeline with mocked steps returning robots_txt_found=True, url_allowed=True, sitemap_urls=["https://example.com/sitemap.xml"]. Assert publisher flat fields updated.
  </action>
  <verify>
Run `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v` -- all tests pass including updated and new supervisor tests.
  </verify>
  <done>
Supervisor runs robots and sitemap steps in sequence after ToS evaluation. Results saved to ResolutionJob. Publisher flat fields updated. Skip events published for fresh publishers. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v` -- all tests pass (existing + new)
2. `uv run python -c "from protego import Protego; print(Protego.parse('User-agent: *\nAllow: /').can_fetch('/', 'bot'))"` -- prints True
3. `uv run pytest --tb=short` -- full test suite passes (no regressions)
</verification>

<success_criteria>
- protego installed and importable
- run_robots_step returns correct results for: found+allowed, found+disallowed, 404, HTML challenge, network error, malformed
- run_sitemap_step returns correct results for: from robots.txt, from probe, none found
- Supervisor runs both steps, saves to ResolutionJob, publishes events, updates publisher fields
- All existing pipeline tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/09-publisher-discovery/09-01-SUMMARY.md`
</output>
