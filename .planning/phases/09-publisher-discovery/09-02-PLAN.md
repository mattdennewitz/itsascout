---
phase: 09-publisher-discovery
plan: 02
type: tdd
wave: 2
depends_on: ["09-01"]
files_modified:
  - scrapegrape/publishers/pipeline/steps.py
  - scrapegrape/publishers/pipeline/supervisor.py
  - scrapegrape/publishers/tests/test_pipeline.py
  - scrapegrape/publishers/views.py
  - scrapegrape/frontend/src/Pages/Jobs/Show.tsx
autonomous: true

must_haves:
  truths:
    - "RSS step discovers feed URLs from HTML <link rel='alternate'> tags with RSS/Atom MIME types"
    - "RSS step resolves relative feed URLs to absolute URLs"
    - "RSS step handles missing homepage HTML gracefully (returns empty feeds list)"
    - "RSL step detects License directives from robots.txt result"
    - "RSL step detects <link rel='license' type='application/rsl+xml'> from homepage HTML"
    - "RSL step detects Link HTTP headers with rel='license' and application/rsl+xml"
    - "RSL step returns rsl_detected=false when no indicators found"
    - "Supervisor fetches homepage HTML once and passes it to both RSS and RSL steps"
    - "Supervisor publishes skip events for rss/rsl when publisher is fresh"
    - "Frontend Shows all 8 pipeline steps with data summaries for new steps"
    - "Job show view passes new result fields (robots_result, sitemap_result, rss_result, rsl_result) to frontend"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/steps.py"
      provides: "run_rss_step, run_rsl_step, FeedLinkParser, RSLLinkParser"
      contains: "run_rss_step"
    - path: "scrapegrape/publishers/pipeline/supervisor.py"
      provides: "RSS and RSL steps integrated, homepage HTML fetch"
      contains: "run_rss_step"
    - path: "scrapegrape/frontend/src/Pages/Jobs/Show.tsx"
      provides: "8 pipeline steps displayed with summaries for all step types"
      contains: "robots"
    - path: "scrapegrape/publishers/views.py"
      provides: "Job show view serializes robots/sitemap/rss/rsl results"
      contains: "robots_result"
  key_links:
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/steps.py"
      via: "import and call run_rss_step, run_rsl_step"
      pattern: "from publishers.pipeline.steps import.*run_rss_step"
    - from: "scrapegrape/publishers/views.py"
      to: "scrapegrape/frontend/src/Pages/Jobs/Show.tsx"
      via: "Inertia props (robots_result, sitemap_result, rss_result, rsl_result)"
      pattern: "robots_result.*job\\.robots_result"
---

<objective>
Add RSS feed discovery, RSL detection pipeline steps, supervisor integration with shared homepage fetch, and update the frontend to display all 8 pipeline steps.

Purpose: Completes the publisher discovery pipeline with RSS and RSL detection, and ensures the user can see all step results in the streaming UI.

Output: Two new step functions (run_rss_step, run_rsl_step), supervisor integration with shared homepage HTML fetch, updated job show view, and updated Show.tsx with 8 pipeline steps.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-publisher-discovery/09-RESEARCH.md
@.planning/phases/09-publisher-discovery/09-01-SUMMARY.md
@scrapegrape/publishers/pipeline/steps.py
@scrapegrape/publishers/pipeline/supervisor.py
@scrapegrape/publishers/tests/test_pipeline.py
@scrapegrape/publishers/views.py
@scrapegrape/frontend/src/Pages/Jobs/Show.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement RSS and RSL step functions with tests</name>
  <files>
    scrapegrape/publishers/pipeline/steps.py
    scrapegrape/publishers/tests/test_pipeline.py
  </files>
  <action>
1. In `steps.py`, add two HTML parsers and two step functions:

**`FeedLinkParser(HTMLParser)`:**
- Extracts `<link>` tags where rel contains "alternate" and type is one of: `application/rss+xml`, `application/atom+xml`, `application/xml`, `text/xml`
- Collects `{"url": href, "type": type, "title": title}` dicts
- Handle both `handle_starttag` and `handle_startendtag` (self-closing `<link ... />`)

**`run_rss_step(publisher, homepage_html: str) -> dict`:**
- Create FeedLinkParser, feed it homepage_html (wrap in try/except for safety)
- Resolve relative URLs with `urljoin(f"https://{publisher.domain}/", feed_url)`
- Return: `{"feeds": [{"url": str, "type": str, "title": str}, ...], "count": int}`
- If homepage_html is empty: return `{"feeds": [], "count": 0, "error": "homepage fetch failed"}`

**`RSLLinkParser(HTMLParser)`:**
- Extracts `<link>` tags where rel contains "license" and type contains "application/rsl+xml"
- Collects href URLs
- Handle both `handle_starttag` and `handle_startendtag`

**`run_rsl_step(publisher, robots_result: dict, homepage_html: str, homepage_headers: dict | None = None) -> dict`:**
- Source 1: Extract license_directives from robots_result (already parsed in robots step)
- Source 2: Parse homepage HTML with RSLLinkParser for `<link rel="license" type="application/rsl+xml">`
- Source 3: Check homepage_headers for Link header containing `rel="license"` and `application/rsl+xml`, extract URL with regex `r'<([^>]+)>'`
- Build indicators list: `[{"source": "robots.txt"|"html_link"|"http_header", "url": str}, ...]`
- Return: `{"rsl_detected": bool, "indicators": list, "count": int}`
- If homepage_html is empty and no robots.txt license directives: return `{"rsl_detected": False, "indicators": [], "count": 0}`

2. In `test_pipeline.py`, add test classes:

**`TestFeedLinkParser`:**
- `test_finds_rss_feed`: HTML with `<link rel="alternate" type="application/rss+xml" href="/feed">`. Assert feed found.
- `test_finds_atom_feed`: HTML with atom+xml type. Assert found.
- `test_ignores_non_feed_links`: HTML with `<link rel="stylesheet">`. Assert no feeds.
- `test_self_closing_link_tag`: HTML with `<link ... />` self-closing. Assert found.

**`TestRunRssStep`:**
- `test_rss_feeds_discovered`: HTML with RSS link. Assert feeds list has entry, count=1.
- `test_relative_url_resolved`: HTML with `href="/feed/rss"`. Assert URL resolved to `https://example.com/feed/rss`.
- `test_empty_html`: Empty string input. Assert count=0, error key present.
- `test_no_feeds_in_html`: HTML without feed links. Assert count=0.
- `test_multiple_feeds`: HTML with both RSS and Atom links. Assert count=2.

**`TestRSLLinkParser`:**
- `test_finds_rsl_link`: HTML with `<link rel="license" type="application/rsl+xml" href="...">`. Assert found.
- `test_ignores_non_rsl_license`: HTML with `<link rel="license" type="text/html">`. Assert not found.

**`TestRunRslStep`:**
- `test_rsl_from_robots_license`: robots_result with license_directives. Assert rsl_detected=True, source="robots.txt".
- `test_rsl_from_html_link`: HTML with RSL link tag, empty robots. Assert rsl_detected=True, source="html_link".
- `test_rsl_from_http_header`: homepage_headers with Link header. Assert rsl_detected=True, source="http_header".
- `test_rsl_all_three_sources`: All three present. Assert count=3.
- `test_rsl_not_detected`: No indicators anywhere. Assert rsl_detected=False, count=0.
- `test_rsl_empty_html_with_robots_license`: Empty HTML but robots has License. Assert rsl_detected=True (robots.txt source still works).

Use PublisherFactory for publisher instances. No monkeypatching needed for these pure-function tests (they take HTML strings directly).
  </action>
  <verify>
Run `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v -k "rss or rsl or Feed or RSL"` -- all new tests pass. Run full test file to ensure no regressions.
  </verify>
  <done>
run_rss_step discovers RSS/Atom feeds from HTML link tags with relative URL resolution. run_rsl_step detects RSL from robots.txt, HTML, and HTTP headers. All new tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate RSS/RSL into supervisor, update job view and frontend</name>
  <files>
    scrapegrape/publishers/pipeline/supervisor.py
    scrapegrape/publishers/tests/test_pipeline.py
    scrapegrape/publishers/views.py
    scrapegrape/frontend/src/Pages/Jobs/Show.tsx
  </files>
  <action>
**1. Supervisor changes (`supervisor.py`):**

Import the new steps:
```python
from publishers.pipeline.steps import (
    run_robots_step,
    run_rss_step,
    run_rsl_step,
    run_sitemap_step,
    ...existing imports...
)
```

Add a homepage HTML fetch helper inside the supervisor (or as a module-level function):
```python
def _fetch_homepage_html(publisher):
    """Fetch publisher homepage HTML. Returns (html, headers) tuple."""
    from publishers.fetchers.manager import FetchStrategyManager
    try:
        manager = FetchStrategyManager()
        result = manager.fetch(f"https://{publisher.domain}/", publisher=publisher)
        return result.html, dict(result.response_headers) if hasattr(result, 'response_headers') else {}
    except Exception as exc:
        logger.warning(f"Could not fetch homepage for {publisher.domain}: {exc}")
        return "", {}
```

NOTE: Check the FetchResult dataclass to see if it has response_headers. If not, just return `("", {})` for the headers part -- the RSL HTTP header check is best-effort.

After the sitemap step (added in plan 09-01), add:

```python
# Fetch homepage HTML once for RSS and RSL steps
homepage_html, homepage_headers = _fetch_homepage_html(publisher)

# Step 6: RSS feed discovery
publish_step_event(job_id, "rss", "started")
rss_result = run_rss_step(publisher, homepage_html)
resolution_job.rss_result = rss_result
resolution_job.save(update_fields=["rss_result"])
publish_step_event(job_id, "rss", "completed", rss_result)

publisher.rss_urls = [f["url"] for f in rss_result.get("feeds", [])]
publisher.save(update_fields=["rss_urls"])

# Step 7: RSL detection
publish_step_event(job_id, "rsl", "started")
rsl_result = run_rsl_step(publisher, robots_result, homepage_html, homepage_headers)
resolution_job.rsl_result = rsl_result
resolution_job.save(update_fields=["rsl_result"])
publish_step_event(job_id, "rsl", "completed", rsl_result)

publisher.rsl_detected = rsl_result.get("rsl_detected", False)
publisher.save(update_fields=["rsl_detected"])
```

In the freshness skip block, add:
```python
publish_step_event(job_id, "rss", "skipped", {"reason": "fresh"})
publish_step_event(job_id, "rsl", "skipped", {"reason": "fresh"})
```

**2. Job show view (`views.py`):**

In the `job_show` function, add the four new result fields to the props dict:
```python
"robots_result": job.robots_result,
"sitemap_result": job.sitemap_result,
"rss_result": job.rss_result,
"rsl_result": job.rsl_result,
```

**3. Frontend (`Show.tsx`):**

Update the `JobProps` interface to include the new result fields:
```typescript
robots_result: Record<string, unknown> | null
sitemap_result: Record<string, unknown> | null
rss_result: Record<string, unknown> | null
rsl_result: Record<string, unknown> | null
```

Update `PIPELINE_STEPS` to include all 8 steps:
```typescript
const PIPELINE_STEPS = [
    { key: 'publisher_resolution', label: 'Publisher Resolution', icon: '1' },
    { key: 'waf', label: 'WAF Detection', icon: '2' },
    { key: 'tos_discovery', label: 'ToS Discovery', icon: '3' },
    { key: 'tos_evaluation', label: 'ToS Evaluation', icon: '4' },
    { key: 'robots', label: 'robots.txt Analysis', icon: '5' },
    { key: 'sitemap', label: 'Sitemap Discovery', icon: '6' },
    { key: 'rss', label: 'RSS Feed Discovery', icon: '7' },
    { key: 'rsl', label: 'RSL Detection', icon: '8' },
] as const
```

Add summary cases in `stepDataSummary`:
```typescript
if (step === 'robots') {
    if (data.robots_found === false) return 'No robots.txt found'
    if (data.url_allowed === true) return 'URL allowed by robots.txt'
    if (data.url_allowed === false) return 'URL disallowed by robots.txt'
    return null
}
if (step === 'sitemap') {
    const count = data.count as number
    if (count > 0) return `Found ${count} sitemap(s)`
    return 'No sitemaps found'
}
if (step === 'rss') {
    const count = data.count as number
    if (count > 0) return `Found ${count} feed(s)`
    return 'No RSS/Atom feeds found'
}
if (step === 'rsl') {
    if (data.rsl_detected) return `RSL detected (${data.count} indicator(s))`
    return 'No RSL licensing detected'
}
```

Update `initialStatuses` in the `useMemo` block to build statuses from new result props for completed jobs:
```typescript
if (job.robots_result) {
    statuses['robots'] = { step: 'robots', status: 'completed', data: job.robots_result }
}
if (job.sitemap_result) {
    statuses['sitemap'] = { step: 'sitemap', status: 'completed', data: job.sitemap_result }
}
if (job.rss_result) {
    statuses['rss'] = { step: 'rss', status: 'completed', data: job.rss_result }
}
if (job.rsl_result) {
    statuses['rsl'] = { step: 'rsl', status: 'completed', data: job.rsl_result }
}
```

**4. Update pipeline tests:**

Update `TestRunPipeline.test_pipeline_runs_all_steps`:
- Add monkeypatches for run_rss_step, run_rsl_step, and _fetch_homepage_html
- Assert "rss" and "rsl" in step_names

Update `TestRunPipeline.test_pipeline_skips_fresh_publisher`:
- Assert ("rss", "skipped") and ("rsl", "skipped") in events_published

Update `TestRunPipeline.test_pipeline_saves_step_results`:
- Add monkeypatches for new steps
- Assert job.rss_result and job.rsl_result are saved

Add test:
- **`test_pipeline_updates_publisher_rss_and_rsl_fields`**: Run pipeline with mocked steps returning feeds and rsl_detected=True. Assert publisher.rss_urls and publisher.rsl_detected updated.
  </action>
  <verify>
1. `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v` -- all tests pass
2. `uv run pytest scrapegrape/publishers/tests/test_views.py -v` -- view tests pass
3. `uv run pytest --tb=short` -- full suite passes
4. Manually inspect Show.tsx: PIPELINE_STEPS has 8 entries, stepDataSummary handles all step keys
  </verify>
  <done>
Supervisor runs RSS and RSL steps with shared homepage HTML fetch. Job show view serializes all 4 new result fields. Frontend displays all 8 pipeline steps with data summaries. Skip events work for fresh publishers. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v` -- all tests pass
2. `uv run pytest --tb=short` -- full test suite passes with no regressions
3. Show.tsx has 8 PIPELINE_STEPS entries
4. job_show view includes robots_result, sitemap_result, rss_result, rsl_result in props
5. stepDataSummary handles robots, sitemap, rss, rsl step keys
</verification>

<success_criteria>
- RSS step discovers feeds from HTML link tags and resolves relative URLs
- RSL step detects indicators from robots.txt, HTML, and HTTP headers
- Homepage HTML fetched once and shared between RSS and RSL steps
- All 8 pipeline steps visible in frontend with appropriate data summaries
- Completed jobs show results from props without needing SSE
- Fresh publisher skip events include all new steps
- Full test suite passes
</success_criteria>

<output>
After completion, create `.planning/phases/09-publisher-discovery/09-02-SUMMARY.md`
</output>
