---
phase: 10-article-metadata
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - scrapegrape/publishers/models.py
  - scrapegrape/publishers/migrations/0007_articlemetadata_and_article_result.py
  - scrapegrape/publishers/pipeline/steps.py
  - scrapegrape/publishers/tests/test_pipeline.py
  - scrapegrape/scrapegrape/settings.py
autonomous: true

must_haves:
  truths:
    - "Article extraction step returns per-format key fields (JSON-LD, OpenGraph, Microdata, Twitter Cards) without merging"
    - "Paywall detection step returns one of four statuses: free, paywalled, metered, unknown"
    - "isAccessibleForFree is checked at top-level and inside hasPart nodes"
    - "Heuristic fallback only reports paywalled when multiple strong signals agree"
    - "Metadata profile step returns LLM-generated human-readable summary"
    - "TwitterCardParser extracts meta name=twitter:* tags via stdlib HTMLParser"
  artifacts:
    - path: "scrapegrape/publishers/models.py"
      provides: "ArticleMetadata model with per-format JSONFields, paywall status, metadata profile"
      contains: "class ArticleMetadata"
    - path: "scrapegrape/publishers/pipeline/steps.py"
      provides: "run_article_extraction_step, run_paywall_detection_step, run_metadata_profile_step"
      exports: ["run_article_extraction_step", "run_paywall_detection_step", "run_metadata_profile_step"]
    - path: "scrapegrape/publishers/tests/test_pipeline.py"
      provides: "Tests for all three article steps"
      contains: "TestRunArticleExtractionStep"
  key_links:
    - from: "scrapegrape/publishers/pipeline/steps.py"
      to: "extruct"
      via: "extruct.extract with syntaxes filter"
      pattern: "extruct\\.extract"
    - from: "scrapegrape/publishers/pipeline/steps.py"
      to: "pydantic_ai"
      via: "Agent for metadata profiling"
      pattern: "metadata_profile_agent"
---

<objective>
Add the ArticleMetadata model, three article pipeline step functions (extraction, paywall detection, metadata profile), and comprehensive tests using TDD.

Purpose: Implements the core article analysis logic that the supervisor will wire in Plan 02. Each step is a pure function returning a dict, following the existing step pattern exactly.
Output: Model, migration, step functions, and passing tests.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-article-metadata/10-CONTEXT.md
@.planning/phases/10-article-metadata/10-RESEARCH.md
@scrapegrape/publishers/models.py
@scrapegrape/publishers/pipeline/steps.py
@scrapegrape/publishers/tests/test_pipeline.py
@scrapegrape/ingestion/terms_discovery.py
@scrapegrape/scrapegrape/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ArticleMetadata model, ResolutionJob article_result field, Publisher has_paywall field, and settings</name>
  <files>scrapegrape/publishers/models.py, scrapegrape/publishers/migrations/0007_articlemetadata_and_article_result.py, scrapegrape/scrapegrape/settings.py</files>
  <action>
1. Add to `Publisher` model:
   - `has_paywall = models.BooleanField(null=True)` -- publisher-level paywall signal, updated after each article analysis. Use latest article's paywall status per research recommendation.

2. Add to `ResolutionJob` model:
   - `article_result = models.JSONField(null=True, blank=True)` -- stores article extraction results (separate from existing `metadata_result` which stores publisher Organization data).

3. Create `ArticleMetadata` model (per research code example):
   - `id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)`
   - `resolution_job = models.ForeignKey("ResolutionJob", on_delete=models.CASCADE, related_name="article_metadata")`
   - `publisher = models.ForeignKey("Publisher", on_delete=models.CASCADE, related_name="article_metadata")`
   - `article_url = models.URLField(db_index=True)`
   - Per-format extracted fields: `jsonld_fields`, `opengraph_fields`, `microdata_fields`, `twitter_cards` -- all `models.JSONField(null=True, blank=True)`
   - Format presence booleans: `has_jsonld`, `has_opengraph`, `has_microdata`, `has_twitter_cards` -- all `models.BooleanField(default=False)`
   - `paywall_status = models.CharField(max_length=20, choices=[("free","Free"),("paywalled","Paywalled (hard)"),("metered","Metered"),("unknown","Unknown")], default="unknown")`
   - `paywall_signals = models.JSONField(default=list, blank=True)`
   - `metadata_profile = models.TextField(blank=True, default="")`
   - `created_at = models.DateTimeField(auto_now_add=True)`
   - Meta: indexes on `["article_url"]` and `["publisher", "created_at"]`

4. Add `ARTICLE_FRESHNESS_TTL = timedelta(hours=24)` to settings.py alongside existing `PUBLISHER_FRESHNESS_TTL`.

5. Run `uv run python manage.py makemigrations publishers` from the `scrapegrape/` directory to generate migration.

6. Run `uv run python manage.py migrate` to apply.
  </action>
  <verify>`uv run python manage.py showmigrations publishers` shows all migrations applied. `uv run python -c "from publishers.models import ArticleMetadata; print(ArticleMetadata._meta.fields)"` succeeds.</verify>
  <done>ArticleMetadata model exists with all fields. ResolutionJob has article_result JSONField. Publisher has has_paywall BooleanField. ARTICLE_FRESHNESS_TTL setting defined. Migration applied.</done>
</task>

<task type="auto">
  <name>Task 2: Three article step functions with TDD (RED-GREEN-REFACTOR)</name>
  <files>scrapegrape/publishers/pipeline/steps.py, scrapegrape/publishers/tests/test_pipeline.py</files>
  <action>
Follow TDD RED-GREEN-REFACTOR cycle. Write tests first, confirm they fail, then implement.

**Step functions to implement (all at bottom of steps.py, following existing pattern):**

### A. `run_article_extraction_step(article_html: str, article_url: str) -> dict`

Extracts metadata from article HTML into per-format sections. Returns:
```python
{
    "jsonld_fields": {...} | None,      # Key fields from JSON-LD article node
    "opengraph_fields": {...} | None,   # Key fields from OpenGraph
    "microdata_fields": {...} | None,   # Key fields from Microdata article node
    "twitter_cards": {...} | None,      # Twitter Card meta tags
    "formats_found": ["json-ld", "opengraph", ...],  # List of formats present
}
```

Implementation details:
- Use `extruct.extract(html, base_url=url, syntaxes=["json-ld", "opengraph", "microdata"], uniform=False)` -- do NOT use uniform=True (loses OG info per research).
- For JSON-LD: Use existing `_flatten_jsonld_nodes()` helper. Check against ARTICLE_TYPES set (Article, NewsArticle, BlogPosting, TechArticle, ScholarlyArticle, OpinionNewsArticle, AnalysisNewsArticle, ReportageNewsArticle, ReviewNewsArticle, LiveBlogPosting, SocialMediaPosting, WebPage, CreativeWork). Extract KEY_FIELDS: headline, author, datePublished, dateModified, image, description, isAccessibleForFree, wordCount, articleSection, inLanguage, keywords. Flatten nested objects (author: {name: "..."} -> just name). Also extract publisher_name from publisher.name if present.
- For OpenGraph: Convert list-of-tuples format from extruct. Map og:title->headline, og:description->description, og:image->image, og:type->type, og:site_name->publisher_name, og:locale->inLanguage, article:published_time->datePublished, article:modified_time->dateModified, article:author->author, article:section->articleSection, article:tag->keywords (accumulate as list).
- For Microdata: Similar to JSON-LD -- find article-type items, extract same KEY_FIELDS.
- For Twitter Cards: Create `TwitterCardParser(HTMLParser)` following existing FeedLinkParser/RSLLinkParser pattern. Extract `<meta name="twitter:*" content="...">` tags. Return dict of {name: content} for all twitter:* tags found.
- Add helper functions as module-level private functions (prefixed with _).

### B. `run_paywall_detection_step(article_html: str, extraction_result: dict) -> dict`

Detects paywall status. Returns:
```python
{
    "paywall_status": "free" | "paywalled" | "metered" | "unknown",
    "signals": [...],           # List of signal strings found
    "schema_accessible": True | False | None,  # isAccessibleForFree value if found
}
```

Implementation details:
- Primary: Check isAccessibleForFree from extraction_result's jsonld_fields. Check BOTH top-level and inside hasPart nodes (Google's nested pattern per research pitfall #4).
- If isAccessibleForFree is explicitly True -> "free". If explicitly False -> "paywalled".
- Fallback heuristics (per research code example): Check login_patterns (subscribe to continue, sign in to read, etc.), paywall CSS classes (paywall, subscriber-only, premium-content, gated-content, meter-, regwall), metered patterns (articles remaining, free articles, monthly limit).
- Decision logic with high confidence bar: metered signals -> "metered". login_wall AND paywall_class -> "paywalled". Zero signals and no schema -> "free" (assume free). Single signal alone -> "unknown".

### C. `run_metadata_profile_step(extraction_result: dict, article_url: str) -> dict`

LLM-generated summary. Returns:
```python
{
    "summary": "Human-readable 2-4 sentence summary...",
    "quality_score": 0.75,
}
```

Implementation details:
- Define `MetadataProfileResult(BaseModel)` with `summary: str` and `quality_score: float` (ge=0.0, le=1.0).
- Define `METADATA_PROFILE_PROMPT` system prompt (per research example).
- Create `metadata_profile_agent = Agent("openai:gpt-4.1-nano", output_type=MetadataProfileResult, system_prompt=METADATA_PROFILE_PROMPT)`.
- Step function calls `agent.run_sync()` with formatted extraction data.
- Wrap in try/except, return `{"summary": "", "quality_score": 0.0, "error": str(exc)}` on failure.

### Tests to write (in test_pipeline.py):

**TestTwitterCardParser:**
- `test_finds_twitter_card_tags` -- HTML with twitter:card, twitter:title, twitter:image -> all extracted
- `test_ignores_non_twitter_meta` -- og:title meta tag not extracted
- `test_self_closing_meta_tag` -- `<meta ... />` works

**TestRunArticleExtractionStep:**
- `test_extracts_jsonld_article` -- HTML with JSON-LD NewsArticle -> jsonld_fields has headline, author, datePublished
- `test_extracts_opengraph` -- HTML with og:title, og:description -> opengraph_fields has headline, description
- `test_extracts_twitter_cards` -- HTML with twitter:card, twitter:title -> twitter_cards dict populated
- `test_extracts_multiple_formats` -- HTML with JSON-LD + OG -> formats_found contains both
- `test_jsonld_graph_nesting` -- JSON-LD with @graph array containing Article -> correctly extracts
- `test_empty_html` -- empty string returns all None fields, empty formats_found
- `test_flattens_nested_author` -- author: {name: "John"} -> fields["author"] == "John"

**TestRunPaywallDetectionStep:**
- `test_schema_accessible_free` -- isAccessibleForFree: true -> "free"
- `test_schema_accessible_paywalled` -- isAccessibleForFree: false -> "paywalled"
- `test_has_part_nested_accessible` -- isAccessibleForFree nested in hasPart -> detected
- `test_heuristic_multiple_signals_paywalled` -- login wall + paywall class -> "paywalled"
- `test_heuristic_single_signal_unknown` -- only login wall pattern -> "unknown"
- `test_heuristic_metered` -- "articles remaining" -> "metered"
- `test_no_signals_free` -- clean HTML with no schema -> "free"

**TestRunMetadataProfileStep:**
- `test_metadata_profile_returns_summary` -- monkeypatch agent.run_sync, verify summary returned
- `test_metadata_profile_handles_error` -- agent raises -> returns error dict

Commit after RED (failing tests): `test(10-01): add failing tests for article extraction, paywall detection, and metadata profiling`
Commit after GREEN (passing): `feat(10-01): implement article extraction, paywall detection, and metadata profile steps`
  </action>
  <verify>`cd /Users/matt/src/itsascout/scrapegrape && uv run pytest publishers/tests/test_pipeline.py -v` -- all tests pass including new article step tests.</verify>
  <done>Three article step functions implemented and tested. TwitterCardParser extracts twitter:* meta tags. Article extraction returns per-format fields without merging. Paywall detection uses schema.org primary + heuristic fallback with high confidence bar. Metadata profile uses pydantic-ai agent with GPT-4.1-nano.</done>
</task>

</tasks>

<verification>
- `uv run pytest publishers/tests/test_pipeline.py -v` -- all tests pass
- ArticleMetadata model has all required fields (per-format JSONFields, paywall_status, metadata_profile)
- `from publishers.pipeline.steps import run_article_extraction_step, run_paywall_detection_step, run_metadata_profile_step` succeeds
- `from publishers.models import ArticleMetadata` succeeds
- ARTICLE_FRESHNESS_TTL is defined in settings
</verification>

<success_criteria>
- ArticleMetadata model created with migration applied
- ResolutionJob.article_result field exists
- Publisher.has_paywall field exists
- Three step functions pass all tests
- Step functions follow existing pattern (pure functions returning dicts)
- No raw extruct dump stored (key fields only, per locked decision)
- Per-format sections kept separate (not merged, per locked decision)
- Twitter Cards extracted via HTMLParser (not extruct, per research finding)
</success_criteria>

<output>
After completion, create `.planning/phases/10-article-metadata/10-01-SUMMARY.md`
</output>
