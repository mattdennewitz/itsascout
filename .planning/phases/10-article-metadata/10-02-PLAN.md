---
phase: 10-article-metadata
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - scrapegrape/publishers/pipeline/supervisor.py
  - scrapegrape/publishers/views.py
  - scrapegrape/publishers/admin.py
  - scrapegrape/frontend/src/Pages/Jobs/Show.tsx
  - scrapegrape/publishers/tests/test_pipeline.py
autonomous: true

must_haves:
  truths:
    - "Pipeline runs article extraction, paywall detection, and metadata profile as steps 10-12 after existing publisher steps"
    - "Article steps are skipped when the same article URL was analyzed within ARTICLE_FRESHNESS_TTL"
    - "Article steps publish SSE events so the frontend shows real-time progress"
    - "ArticleMetadata record is created and saved with extraction results"
    - "Publisher.has_paywall is updated from article paywall status"
    - "Job show page displays article steps with summaries"
    - "Step summary for extraction shows field count; paywall shows status; profile shows truncated summary"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/supervisor.py"
      provides: "Article steps wired into run_pipeline"
      contains: "run_article_extraction_step"
    - path: "scrapegrape/frontend/src/Pages/Jobs/Show.tsx"
      provides: "Three new step cards for article analysis"
      contains: "article_extraction"
    - path: "scrapegrape/publishers/views.py"
      provides: "article_result in job_show props"
      contains: "article_result"
  key_links:
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/steps.py"
      via: "imports and calls article step functions"
      pattern: "run_article_extraction_step"
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/models.py"
      via: "creates ArticleMetadata record"
      pattern: "ArticleMetadata\\.objects\\.create"
    - from: "scrapegrape/frontend/src/Pages/Jobs/Show.tsx"
      to: "scrapegrape/publishers/views.py"
      via: "job props include article_result"
      pattern: "article_result"
---

<objective>
Wire the three article steps into the pipeline supervisor, update the frontend to display article analysis progress, and add supervisor-level tests.

Purpose: Connects the step functions from Plan 01 into the live pipeline so users see article metadata extraction, paywall detection, and LLM profiling in real time.
Output: Working end-to-end article analysis pipeline with frontend display.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-article-metadata/10-CONTEXT.md
@.planning/phases/10-article-metadata/10-RESEARCH.md
@.planning/phases/10-article-metadata/10-01-SUMMARY.md
@scrapegrape/publishers/pipeline/supervisor.py
@scrapegrape/publishers/views.py
@scrapegrape/publishers/admin.py
@scrapegrape/frontend/src/Pages/Jobs/Show.tsx
@scrapegrape/publishers/tests/test_pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire article steps into supervisor with freshness TTL and ArticleMetadata creation</name>
  <files>scrapegrape/publishers/pipeline/supervisor.py, scrapegrape/publishers/views.py, scrapegrape/publishers/admin.py, scrapegrape/publishers/tests/test_pipeline.py</files>
  <action>
**Supervisor changes (supervisor.py):**

1. Import the three new step functions:
   ```python
   from publishers.pipeline.steps import (
       ...,  # existing imports
       run_article_extraction_step,
       run_paywall_detection_step,
       run_metadata_profile_step,
   )
   from publishers.models import ResolutionJob, ArticleMetadata
   ```

2. Add article freshness check helper:
   ```python
   def _should_skip_article_steps(article_url: str) -> bool:
       """Return True if this article URL was analyzed within ARTICLE_FRESHNESS_TTL."""
       from django.conf import settings
       recent = ArticleMetadata.objects.filter(
           article_url=article_url,
           created_at__gte=timezone.now() - settings.ARTICLE_FRESHNESS_TTL,
       ).first()
       return recent is not None
   ```

3. After the existing publisher steps block (after `publisher.save(update_fields=["last_checked_at"])`) but before "Mark job complete", add article steps:

   ```python
   # --- Article-level steps ---
   article_url = resolution_job.canonical_url

   if _should_skip_article_steps(article_url):
       publish_step_event(job_id, "article_extraction", "skipped", {"reason": "fresh"})
       publish_step_event(job_id, "paywall_detection", "skipped", {"reason": "fresh"})
       publish_step_event(job_id, "metadata_profile", "skipped", {"reason": "fresh"})
   else:
       # Fetch article HTML (reuse homepage_html if article URL matches homepage)
       homepage_url = publisher.url or f"https://{publisher.domain}/"
       if article_url.rstrip("/") == homepage_url.rstrip("/"):
           article_html = homepage_html  # Already fetched above
       else:
           try:
               fetch_result = _fetch_manager.fetch(article_url, publisher=publisher)
               article_html = fetch_result.html
           except AllStrategiesExhausted as exc:
               logger.warning(f"Could not fetch article {article_url}: {exc}")
               article_html = ""

       # Step 10: Article extraction
       publish_step_event(job_id, "article_extraction", "started")
       extraction_result = run_article_extraction_step(article_html, article_url)

       # Step 11: Paywall detection
       publish_step_event(job_id, "paywall_detection", "started")
       paywall_result = run_paywall_detection_step(article_html, extraction_result)

       # Step 12: Metadata profile
       publish_step_event(job_id, "metadata_profile", "started")
       profile_result = run_metadata_profile_step(extraction_result, article_url)

       # Combine into article_result
       article_result = {
           **extraction_result,
           "paywall": paywall_result,
           "profile": profile_result,
       }
       resolution_job.article_result = article_result
       resolution_job.save(update_fields=["article_result"])

       # Create ArticleMetadata record
       ArticleMetadata.objects.create(
           resolution_job=resolution_job,
           publisher=publisher,
           article_url=article_url,
           jsonld_fields=extraction_result.get("jsonld_fields"),
           opengraph_fields=extraction_result.get("opengraph_fields"),
           microdata_fields=extraction_result.get("microdata_fields"),
           twitter_cards=extraction_result.get("twitter_cards"),
           has_jsonld=bool(extraction_result.get("jsonld_fields")),
           has_opengraph=bool(extraction_result.get("opengraph_fields")),
           has_microdata=bool(extraction_result.get("microdata_fields")),
           has_twitter_cards=bool(extraction_result.get("twitter_cards")),
           paywall_status=paywall_result.get("paywall_status", "unknown"),
           paywall_signals=paywall_result.get("signals", []),
           metadata_profile=profile_result.get("summary", ""),
       )

       # Update publisher-level paywall signal (latest article's status)
       publisher.has_paywall = paywall_result.get("paywall_status") in ("paywalled", "metered")
       publisher.save(update_fields=["has_paywall"])

       # Publish completion events with summaries
       # Extraction summary: list field names found
       fields_found = extraction_result.get("formats_found", [])
       extraction_summary = f"{len(fields_found)} format(s): {', '.join(fields_found)}" if fields_found else "No structured data found"
       publish_step_event(job_id, "article_extraction", "completed", {**extraction_result, "summary": extraction_summary})

       # Paywall summary
       paywall_summary = f"Status: {paywall_result.get('paywall_status', 'unknown')}"
       if paywall_result.get("schema_accessible") is not None:
           paywall_summary += f" (isAccessibleForFree: {paywall_result['schema_accessible']})"
       publish_step_event(job_id, "paywall_detection", "completed", {**paywall_result, "summary": paywall_summary})

       # Profile summary (first ~50 chars)
       profile_summary_text = profile_result.get("summary", "")[:50]
       if len(profile_result.get("summary", "")) > 50:
           profile_summary_text += "..."
       publish_step_event(job_id, "metadata_profile", "completed", {**profile_result, "summary": profile_summary_text})
   ```

4. Also add the three article step skip events to the publisher freshness TTL skip block (where all publisher steps are skipped):
   ```python
   publish_step_event(job_id, "article_extraction", "skipped", {"reason": "fresh"})
   publish_step_event(job_id, "paywall_detection", "skipped", {"reason": "fresh"})
   publish_step_event(job_id, "metadata_profile", "skipped", {"reason": "fresh"})
   ```

**Views changes (views.py):**

In `job_show()`, add `article_result` to the job props dict:
```python
"article_result": job.article_result,
```

**Admin changes (admin.py):**

Register ArticleMetadata in admin with basic list display:
```python
from .models import Publisher, ResolutionJob, WAFReport, ArticleMetadata

@admin.register(ArticleMetadata)
class ArticleMetadataAdmin(admin.ModelAdmin):
    list_display = ["article_url", "publisher", "paywall_status", "has_jsonld", "has_opengraph", "has_microdata", "has_twitter_cards", "created_at"]
    list_filter = ["paywall_status", "has_jsonld", "has_opengraph", "has_microdata", "has_twitter_cards"]
    search_fields = ["article_url", "publisher__name"]
    readonly_fields = ["id", "created_at"]
```

**Test changes (test_pipeline.py):**

Add to existing `TestRunPipeline` class:

- `test_pipeline_runs_article_steps`: Monkeypatch all publisher steps AND article steps. Verify events published for article_extraction, paywall_detection, metadata_profile. Verify job.article_result is populated. Verify ArticleMetadata record created.

- `test_pipeline_skips_fresh_article`: Create an ArticleMetadata with same article_url and recent created_at. Verify article steps are skipped (skip events published, step functions not called).

- `test_pipeline_reuses_homepage_html_for_article`: When canonical_url matches homepage URL, verify _fetch_manager.fetch is NOT called a second time for article HTML (count fetch calls).

Update existing `test_pipeline_runs_all_steps` to also monkeypatch the three new article step functions and verify their events appear.

Update existing `test_pipeline_skips_fresh_publisher` to also verify article skip events are published.
  </action>
  <verify>`cd /Users/matt/src/itsascout/scrapegrape && uv run pytest publishers/tests/test_pipeline.py -v` -- all tests pass including new supervisor tests.</verify>
  <done>Pipeline runs 12 steps total. Article steps 10-12 execute after publisher steps. Freshness TTL skips article steps when recently analyzed. ArticleMetadata record saved. Publisher.has_paywall updated. article_result included in job props. ArticleMetadata registered in admin.</done>
</task>

<task type="auto">
  <name>Task 2: Frontend article step cards in Jobs/Show</name>
  <files>scrapegrape/frontend/src/Pages/Jobs/Show.tsx</files>
  <action>
Update Jobs/Show.tsx to display the three new article analysis steps:

1. Add three new entries to `PIPELINE_STEPS` array after the existing 9 steps:
   ```typescript
   { key: 'article_extraction', label: 'Article Metadata', icon: '10' },
   { key: 'paywall_detection', label: 'Paywall Detection', icon: '11' },
   { key: 'metadata_profile', label: 'Metadata Profile', icon: '12' },
   ```

2. Add `article_result` to the `JobProps` interface:
   ```typescript
   article_result: Record<string, unknown> | null
   ```

3. Add `stepDataSummary` cases for the three new steps:
   ```typescript
   if (step === 'article_extraction') {
       const formats = data.formats_found as string[] | undefined
       if (formats && formats.length > 0) return `Found: ${formats.join(', ')}`
       if (data.summary) return String(data.summary)
       return 'No structured data found'
   }
   if (step === 'paywall_detection') {
       const status = data.paywall_status as string | undefined
       if (status) {
           const label: Record<string, string> = {
               free: 'Free access',
               paywalled: 'Paywalled (hard)',
               metered: 'Metered access',
               unknown: 'Unknown',
           }
           let result = label[status] ?? status
           if (data.schema_accessible !== undefined && data.schema_accessible !== null) {
               result += ` (isAccessibleForFree: ${data.schema_accessible})`
           }
           return result
       }
       if (data.summary) return String(data.summary)
       return null
   }
   if (step === 'metadata_profile') {
       if (data.summary) return String(data.summary)
       return null
   }
   ```

4. Update the `initialStatuses` useMemo to handle article_result for completed jobs:
   ```typescript
   if (job.article_result) {
       const ar = job.article_result as Record<string, unknown>
       statuses['article_extraction'] = {
           step: 'article_extraction',
           status: 'completed',
           data: {
               formats_found: ar.formats_found,
               jsonld_fields: ar.jsonld_fields,
               opengraph_fields: ar.opengraph_fields,
               microdata_fields: ar.microdata_fields,
               twitter_cards: ar.twitter_cards,
           },
       }
       const paywall = ar.paywall as Record<string, unknown> | undefined
       if (paywall) {
           statuses['paywall_detection'] = {
               step: 'paywall_detection',
               status: 'completed',
               data: paywall,
           }
       }
       const profile = ar.profile as Record<string, unknown> | undefined
       if (profile) {
           statuses['metadata_profile'] = {
               step: 'metadata_profile',
               status: 'completed',
               data: profile,
           }
       }
   }
   ```

5. Add a visual separator between publisher steps and article steps in the step cards section. Insert a small heading before the article steps:
   ```tsx
   <div className="space-y-3">
       {PIPELINE_STEPS.slice(0, 9).map((step) => (
           <StepCard key={step.key} step={step} event={mergedStatuses[step.key]} />
       ))}

       <div className="pt-2 pb-1">
           <h3 className="text-xs font-medium text-gray-500 uppercase tracking-wide">Article Analysis</h3>
       </div>

       {PIPELINE_STEPS.slice(9).map((step) => (
           <StepCard key={step.key} step={step} event={mergedStatuses[step.key]} />
       ))}
   </div>
   ```
  </action>
  <verify>`cd /Users/matt/src/itsascout/scrapegrape && npm run --prefix frontend build` succeeds with no TypeScript errors.</verify>
  <done>Jobs/Show displays 12 pipeline steps with article analysis section. Step summaries show format list for extraction, paywall status for detection, and truncated LLM summary for profiling. Completed jobs reconstruct article step statuses from article_result props.</done>
</task>

</tasks>

<verification>
- `uv run pytest publishers/tests/test_pipeline.py -v` -- all tests pass
- `npm run --prefix frontend build` -- no TypeScript errors
- Pipeline supervisor calls all 12 steps sequentially
- ArticleMetadata created in DB after article analysis
- Job show page renders 12 step cards with article analysis header
- article_result included in job props
- Freshness TTL skips article steps correctly
</verification>

<success_criteria>
- Three article steps (10, 11, 12) execute after nine publisher steps in pipeline
- Article freshness TTL prevents redundant analysis of same URL
- Homepage HTML reused when article URL matches homepage (per locked decision)
- SSE events published for all article steps (started, completed, skipped)
- ArticleMetadata record created with per-format fields and paywall status
- Publisher.has_paywall updated from latest article
- Frontend shows article analysis steps with step-specific summaries
- Step summaries match locked decisions: extraction shows field count, paywall shows status, profile shows truncated summary
</success_criteria>

<output>
After completion, create `.planning/phases/10-article-metadata/10-02-SUMMARY.md`
</output>
