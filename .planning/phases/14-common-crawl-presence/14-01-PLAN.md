---
phase: 14-common-crawl-presence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scrapegrape/publishers/pipeline/steps.py
  - scrapegrape/publishers/pipeline/supervisor.py
  - scrapegrape/publishers/tests/test_pipeline.py
autonomous: true

must_haves:
  truths:
    - "CC step queries CDX Index API with domain wildcard and reports presence/absence"
    - "When present, result includes estimated page count and latest crawl date"
    - "CC API failures or timeouts produce available=False with error string, never a pipeline crash"
    - "CC step emits SSE started/completed events and saves result to ResolutionJob.cc_result"
    - "TTL skip path copies cc_result from prior job and emits skipped event"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/steps.py"
      provides: "run_cc_step function"
      contains: "def run_cc_step"
    - path: "scrapegrape/publishers/pipeline/supervisor.py"
      provides: "CC step wiring with SSE events and TTL skip"
      contains: "run_cc_step"
    - path: "scrapegrape/publishers/tests/test_pipeline.py"
      provides: "CC step unit tests"
      contains: "TestRunCCStep"
  key_links:
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/steps.py"
      via: "import and call run_cc_step"
      pattern: "run_cc_step\\(publisher\\)"
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "ResolutionJob.cc_result"
      via: "save result to JSONField"
      pattern: "resolution_job\\.cc_result"
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "Publisher flat fields"
      via: "update cc_in_index, cc_page_count, cc_last_crawl"
      pattern: "publisher\\.cc_in_index"
---

<objective>
Add Common Crawl presence detection to the pipeline.

Purpose: Users see whether their publisher domain appears in Common Crawl and how extensively it has been crawled. This is the first competitive intelligence signal (CC-01 through CC-04).

Output: Working `run_cc_step` function querying CC CDX Index API, wired into supervisor with SSE events, TTL skip path, and comprehensive tests.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-common-crawl-presence/14-RESEARCH.md
@scrapegrape/publishers/pipeline/steps.py
@scrapegrape/publishers/pipeline/supervisor.py
@scrapegrape/publishers/pipeline/events.py
@scrapegrape/publishers/models.py
@scrapegrape/publishers/tests/test_pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add run_cc_step function and tests</name>
  <files>
    scrapegrape/publishers/pipeline/steps.py
    scrapegrape/publishers/tests/test_pipeline.py
  </files>
  <action>
Add `run_cc_step(publisher: Publisher) -> dict` to the end of `steps.py` (before the metadata profile section or at the very end, following the existing section comment pattern with a `# ---------------------------------------------------------------------------` block).

The function must:

1. Import `httpx` at the top of the function (lazy import like `extruct` in existing steps, to avoid import-time issues). Use the CC CDX API endpoint format: `https://index.commoncrawl.org/CC-MAIN-2026-04-index`

2. **Request 1 -- Presence check:** GET `{endpoint}?url=*.{publisher.domain}&output=json&showNumPages=true` with a 15-second timeout via `httpx.get(url, timeout=15.0)`. Parse the JSON response to get `pages` and `blocks` fields. If `pages == 0` or the response is empty, return `{"available": True, "in_index": False, "page_count": 0, "latest_crawl": None, "collection": "CC-MAIN-2026-04", "error": None}`.

3. **Request 2 -- Latest timestamp:** If pages > 0, GET `{endpoint}?url=*.{publisher.domain}&output=json&fl=timestamp&limit=1&sort=desc` with 15-second timeout. Parse the first line of newline-delimited JSON to extract the `timestamp` field. Convert CC timestamp format (YYYYMMDDhhmmss) to `YYYY-MM` for `latest_crawl`.

4. **Estimate page count** from `blocks * 3000` (each block is approximately 3000 records per research).

5. **Return dict shape:**
   ```python
   {
       "available": True,
       "in_index": True,
       "page_count": estimated_count,
       "latest_crawl": "YYYY-MM",
       "collection": "CC-MAIN-2026-04",
       "error": None,
   }
   ```

6. **Error handling:** Wrap the entire function body in try/except catching `Exception`. On any error (timeout, connection, JSON parse, etc.), log with `logger.error(...)` and return `{"available": False, "in_index": None, "page_count": None, "latest_crawl": None, "collection": "CC-MAIN-2026-04", "error": str(exc)}`. This ensures CC failures NEVER crash the pipeline (CC-03).

Add tests in `test_pipeline.py` as a new `TestRunCCStep` class (following the existing class pattern, e.g. `TestRunWafStep`). All tests need `@pytest.mark.django_db` on the class. Use `monkeypatch.setattr("publishers.pipeline.steps.httpx", mock_httpx)` pattern to mock httpx at module level.

Tests to add:
- `test_cc_step_domain_found`: Mock httpx.get to return `{"pages": 5, "pageSize": 5, "blocks": 15}` for showNumPages request, and `{"timestamp": "20260115120000"}` (as a line of text) for limit=1 request. Assert result has `in_index=True`, `page_count=45000`, `latest_crawl="2026-01"`.
- `test_cc_step_domain_not_found`: Mock httpx.get to return `{"pages": 0, "pageSize": 0, "blocks": 0}` for showNumPages. Assert result has `in_index=False`, `page_count=0`.
- `test_cc_step_api_timeout`: Mock httpx.get to raise `httpx.TimeoutException("timeout")`. Assert result has `available=False`, `error` contains "timeout", and no exception propagates.
- `test_cc_step_malformed_response`: Mock httpx.get to return non-JSON text. Assert result has `available=False` and `error` is a string.

For the mock pattern: create a mock httpx module object with a `get` method that returns a mock response object with `.text` attribute and `.raise_for_status()` method. Use `monkeypatch.setattr("publishers.pipeline.steps.httpx", mock_httpx_module)` since httpx will be imported at module level.

Actually, since httpx should be imported at the top of steps.py (not lazy -- it's lightweight and already a dependency), add `import httpx` to the imports at the top of steps.py. This makes monkeypatching straightforward: `monkeypatch.setattr("publishers.pipeline.steps.httpx.get", mock_get_fn)` or mock the entire httpx module.
  </action>
  <verify>
Run: `uv run pytest scrapegrape/publishers/tests/test_pipeline.py::TestRunCCStep -v`
All 4 tests pass. No other tests broken: `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v`
  </verify>
  <done>
run_cc_step function exists in steps.py, queries CC CDX API with domain wildcard, returns structured dict with page_count and latest_crawl, handles all error cases gracefully, and has 4 passing tests.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire CC step into supervisor with SSE events and TTL skip</name>
  <files>
    scrapegrape/publishers/pipeline/supervisor.py
    scrapegrape/publishers/tests/test_pipeline.py
  </files>
  <action>
Wire `run_cc_step` into the pipeline supervisor. Three changes needed:

**1. Import:** Add `run_cc_step` to the import block from `publishers.pipeline.steps` in supervisor.py.

**2. Add CC step in the main pipeline flow.** Place it after Step 7 (RSL detection, line ~250) and before Step 8 (Publisher details). Following the exact pattern of other steps:
```python
# Step: Common Crawl presence
publish_step_event(job_id, "cc", "started")
cc_result = run_cc_step(publisher)
resolution_job.cc_result = cc_result
resolution_job.save(update_fields=["cc_result"])
publish_step_event(job_id, "cc", "completed", cc_result)

# Update publisher flat fields
publisher.cc_in_index = cc_result.get("in_index")
publisher.cc_page_count = cc_result.get("page_count")
publisher.cc_last_crawl = cc_result.get("latest_crawl") or ""
publisher.save(update_fields=["cc_in_index", "cc_page_count", "cc_last_crawl"])
```

**3. Update TTL skip path.** In the `should_skip_publisher_steps` branch (around line 94-147):
- Add `"cc_result"` to the `.values()` query (the list that currently has `"waf_result", "tos_result", ...`)
- Add `resolution_job.cc_result = prior["cc_result"]` to the copy block
- Add `"cc_result"` to the `save(update_fields=[...])` call
- Add `publish_step_event(job_id, "cc", "skipped", {"reason": "fresh"})` to the skip events block (place it after the existing "rsl" skipped event and before "publisher_details" skipped event)

**4. Update existing pipeline tests.** The existing `test_pipeline_runs_all_steps` and `test_pipeline_skips_fresh_publisher` tests will need updates:
- In `test_pipeline_runs_all_steps`: add a monkeypatch for `run_cc_step` returning a mock result dict (e.g., `{"available": True, "in_index": True, "page_count": 45000, "latest_crawl": "2026-01", "collection": "CC-MAIN-2026-04", "error": None}`). Verify the job's `cc_result` is saved and publisher flat fields are updated.
- In `test_pipeline_skips_fresh_publisher`: add `"cc_result"` to the prior job's result fields so it gets copied. Verify `cc_result` appears on the new job after skip. Add "cc" to the expected skipped events.
- In `test_pipeline_saves_step_results`: add cc_result mock and assertion.
- Any other pipeline integration test that sets up all step mocks will need a `run_cc_step` mock added.

Look at how existing steps are mocked in these tests (monkeypatch on `publishers.pipeline.supervisor.run_waf_step` etc.) and follow the same pattern for `publishers.pipeline.supervisor.run_cc_step`.
  </action>
  <verify>
Run full test suite: `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v`
All tests pass including updated integration tests. Specifically verify:
- `test_pipeline_runs_all_steps` passes with CC step included
- `test_pipeline_skips_fresh_publisher` passes with cc_result copied
  </verify>
  <done>
CC step is wired into the supervisor pipeline flow with SSE started/completed events, saves to ResolutionJob.cc_result, updates Publisher flat fields (cc_in_index, cc_page_count, cc_last_crawl), TTL skip path copies cc_result and emits "cc" skipped event, all existing and new tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest scrapegrape/publishers/tests/test_pipeline.py -v` -- all tests pass
2. `uv run python manage.py check` -- no Django system check errors
3. Grep for `run_cc_step` in supervisor.py confirms it's called in pipeline flow
4. Grep for `"cc"` in supervisor skip block confirms TTL skip emits cc skipped event
5. Grep for `"cc_result"` in supervisor skip block confirms prior result is copied
</verification>

<success_criteria>
- run_cc_step queries CC CDX API with `*.{domain}` wildcard match (CC-02)
- Result includes in_index, page_count, latest_crawl (CC-01, CC-04)
- All exceptions caught gracefully, returning available=False (CC-03)
- Supervisor emits SSE cc/started and cc/completed events
- TTL skip path copies cc_result and emits cc/skipped event
- Publisher flat fields cc_in_index, cc_page_count, cc_last_crawl updated
- All pipeline tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-common-crawl-presence/14-01-SUMMARY.md`
</output>
