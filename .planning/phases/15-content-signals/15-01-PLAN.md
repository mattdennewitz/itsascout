---
phase: 15-content-signals
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - scrapegrape/publishers/pipeline/steps.py
  - scrapegrape/publishers/tests/test_pipeline.py
autonomous: true

must_haves:
  truths:
    - "run_sitemap_analysis_step detects xmlns:news namespace in sitemap XML"
    - "run_sitemap_analysis_step follows sitemap index children to find news sitemaps"
    - "run_sitemap_analysis_step extracts lastmod dates from sitemaps for frequency fallback"
    - "run_frequency_step parses RSS publication dates via feedparser and computes median interval"
    - "run_frequency_step falls back to sitemap lastmod dates when RSS unavailable"
    - "Frequency estimate includes confidence indicator (high/medium/low) based on sample size and date span"
    - "Both steps return structured dicts matching the established step function pattern"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/steps.py"
      provides: "run_sitemap_analysis_step, run_frequency_step, and helper functions"
      contains: "def run_sitemap_analysis_step"
    - path: "scrapegrape/publishers/tests/test_pipeline.py"
      provides: "Tests for sitemap analysis and frequency step functions"
      contains: "TestSitemapAnalysisStep"
  key_links:
    - from: "scrapegrape/publishers/pipeline/steps.py"
      to: "feedparser"
      via: "import feedparser"
      pattern: "feedparser\\.parse"
    - from: "scrapegrape/publishers/pipeline/steps.py"
      to: "xml.etree.ElementTree"
      via: "import xml.etree.ElementTree"
      pattern: "ET\\.fromstring"
---

<objective>
Add two new pipeline step functions for content signal detection: sitemap analysis (news namespace detection + lastmod extraction) and update frequency estimation (RSS dates via feedparser with sitemap lastmod fallback). Install feedparser dependency.

Purpose: These step functions produce the data needed for the Content Signals section of the report card -- news sitemap presence and publishing frequency with confidence indicator.
Output: Two tested step functions in steps.py, feedparser 6.0.12 installed.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-content-signals/15-RESEARCH.md
@scrapegrape/publishers/pipeline/steps.py
@scrapegrape/publishers/models.py
@scrapegrape/publishers/tests/test_pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install feedparser and add step functions</name>
  <files>
    pyproject.toml
    scrapegrape/publishers/pipeline/steps.py
  </files>
  <action>
1. Install feedparser:
   ```
   uv add feedparser==6.0.12
   ```

2. Add to steps.py imports (at top, with existing imports):
   ```python
   import xml.etree.ElementTree as ET
   import feedparser
   from datetime import datetime, timezone as dt_timezone
   from time import mktime
   from statistics import median
   ```

3. Add helper function `_extract_sitemap_locs(xml_text, limit=2)`:
   - Parse XML with ET.fromstring, catch ET.ParseError and return []
   - Use namespace {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}
   - Find "sm:sitemap/sm:loc" elements, return up to `limit` .text.strip() values

4. Add helper function `_extract_lastmod_dates(xml_text, limit=50)`:
   - Parse XML with ET.fromstring, catch ET.ParseError and return []
   - Use same sitemap namespace
   - Find "sm:url/sm:lastmod" elements, return up to `limit` .text.strip() values

5. Add `run_sitemap_analysis_step(publisher: Publisher) -> dict`:
   - If publisher.sitemap_urls is empty/None, return early with has_news_sitemap=False, empty lastmod_dates
   - Loop over first 3 sitemap URLs from publisher.sitemap_urls
   - For each: fetch via `_fetch_manager.fetch(url, publisher=publisher)`, get result.html
   - Check for xmlns:news namespace via string search: `"xmlns:news" in xml_text or "schemas/sitemap-news" in xml_text`
   - If `<sitemapindex` found in response, extract child URLs with _extract_sitemap_locs(limit=2), prioritize URLs containing "news" in the name (sort those first). Fetch each child and check for xmlns:news and extract lastmod dates.
   - For regular sitemaps, extract lastmod dates with _extract_lastmod_dates
   - Return dict: {has_news_sitemap: bool, news_sitemap_url: str|None, sitemaps_checked: int, lastmod_dates: list[str] (sorted desc, max 50), error: None}
   - Wrap each fetch in try/except (continue on failure, log with logger.error)

6. Add helper `_extract_rss_dates(feed_url: str) -> list[datetime]`:
   - Fetch with httpx.get(feed_url, timeout=10.0, follow_redirects=True), raise_for_status
   - Parse with feedparser.parse(resp.text)
   - For each entry: get entry.get("published_parsed") or entry.get("updated_parsed")
   - Convert to datetime via datetime.fromtimestamp(mktime(parsed), tz=dt_timezone.utc)
   - Return sorted(dates, reverse=True), catch all exceptions and return []

7. Add helper `_compute_frequency(dates: list[datetime], source: str) -> dict`:
   - If < 2 dates, return dict with frequency_label="", confidence="low", sample_size=len(dates)
   - Compute intervals_hours between consecutive dates (abs delta in hours), skip zero deltas
   - Compute median of intervals_hours
   - Compute span_days from first to last date
   - Confidence: high if >= 10 items AND span >= 7 days, medium if >= 5 AND span >= 3, else low
   - Format label with _format_frequency_label(med_hours)
   - Return {source, frequency_label, frequency_hours: round(med_hours,1), confidence, sample_size: len(dates), date_span_days: round(span_days,1), error: None}

8. Add helper `_format_frequency_label(hours_between: float) -> str`:
   - posts_per_day = 24 / hours_between (guard hours_between <= 0 -> "~multiple/hour")
   - >= 2/day: f"~{round(posts_per_day)} articles/day"
   - >= 1/day: "~1 article/day"
   - >= 1/week: f"~{round(posts_per_day * 7)} articles/week"
   - >= 1/month: f"~{round(posts_per_day * 30)} articles/month"
   - else: "< 1 article/month"

9. Add helper `_parse_lastmod_dates(date_strings: list[str]) -> list[datetime]`:
   - For each string, try datetime.fromisoformat(s.replace("Z", "+00:00"))
   - If no timezone info, assume UTC
   - Return sorted(dates, reverse=True), skip unparseable strings

10. Add `run_frequency_step(publisher: Publisher, sitemap_analysis_result: dict | None = None) -> dict`:
    - Try RSS first: if publisher.rss_urls has entries, call _extract_rss_dates(rss_urls[0])
    - If >= 2 RSS dates, return _compute_frequency(dates, source="rss")
    - Fallback: get lastmod_dates from sitemap_analysis_result dict
    - Parse lastmod_dates with _parse_lastmod_dates
    - If >= 2 parsed dates, return _compute_frequency(parsed, source="sitemap")
    - If nothing: return {source: "none", frequency_label: "", frequency_hours: None, confidence: "low", sample_size: 0, date_span_days: 0, error: None}
  </action>
  <verify>
    `uv run python -c "import feedparser; print(feedparser.__version__)"` prints 6.0.12.
    `uv run python -c "from publishers.pipeline.steps import run_sitemap_analysis_step, run_frequency_step; print('OK')"` prints OK.
  </verify>
  <done>
    feedparser 6.0.12 installed. run_sitemap_analysis_step and run_frequency_step exist in steps.py with all helpers. Both follow the established step function pattern (take Publisher, return dict).
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for both step functions</name>
  <files>scrapegrape/publishers/tests/test_pipeline.py</files>
  <action>
Add two new test classes to test_pipeline.py:

**TestSitemapAnalysisStep** (all @pytest.mark.django_db):

1. `test_no_sitemaps_returns_empty`: Publisher with sitemap_urls=[], verify has_news_sitemap=False, sitemaps_checked=0, lastmod_dates=[]

2. `test_detects_news_namespace`: Monkeypatch _fetch_manager.fetch to return FetchResult with XML containing `xmlns:news="http://www.google.com/schemas/sitemap-news/0.9"` in a urlset. Verify has_news_sitemap=True, news_sitemap_url set correctly.

3. `test_detects_news_in_sitemap_index_child`: Monkeypatch _fetch_manager.fetch to return a sitemapindex XML on first call, then a regular sitemap with xmlns:news on second call. Verify has_news_sitemap=True, sitemaps_checked >= 2.

4. `test_extracts_lastmod_dates`: Monkeypatch _fetch_manager.fetch to return XML with 3 `<url><lastmod>2026-02-15</lastmod></url>` entries. Verify lastmod_dates contains the dates.

5. `test_fetch_error_continues`: Monkeypatch _fetch_manager.fetch to raise Exception on first URL but succeed on second. Publisher has 2 sitemap_urls. Verify sitemaps_checked=1 (second one succeeded).

**TestFrequencyStep** (all @pytest.mark.django_db):

1. `test_rss_frequency_high_confidence`: Monkeypatch httpx.get (in steps module) to return response with RSS XML containing 15 entries with published dates spanning 14 days. Use feedparser-compatible RSS format. Verify source="rss", confidence="high", frequency_label contains "articles/day" or similar, sample_size=15.

2. `test_rss_fallback_to_sitemap_lastmod`: Publisher with rss_urls=[]. Pass sitemap_analysis_result with lastmod_dates containing 10 ISO date strings spanning 10 days. Verify source="sitemap", confidence="high".

3. `test_no_rss_no_sitemap_returns_none`: Publisher with rss_urls=[], sitemap_analysis_result=None. Verify source="none", confidence="low", sample_size=0.

4. `test_low_confidence_few_samples`: Monkeypatch httpx.get to return RSS with 3 entries over 1 day. Verify confidence="low".

5. `test_medium_confidence`: Monkeypatch httpx.get to return RSS with 6 entries spanning 4 days. Verify confidence="medium".

6. `test_format_frequency_label_daily`: Import _format_frequency_label directly. Test: 8 hours -> "~3 articles/day", 24 hours -> "~1 article/day", 72 hours -> "~2 articles/week", 720 hours -> "~1 articles/month".

For RSS test data, build minimal valid RSS XML:
```xml
&lt;?xml version="1.0"?&gt;
&lt;rss version="2.0"&gt;&lt;channel&gt;
  &lt;item&gt;&lt;pubDate&gt;Mon, 17 Feb 2026 12:00:00 GMT&lt;/pubDate&gt;&lt;/item&gt;
  ...
&lt;/channel&gt;&lt;/rss&gt;
```

For monkeypatching httpx.get in steps module: monkeypatch.setattr("publishers.pipeline.steps.httpx.get", mock_fn) or create a mock response object with .text and .status_code=200 and .raise_for_status as no-op.
  </action>
  <verify>
    `cd /Users/matt/src/itsascout/scrapegrape && uv run pytest publishers/tests/test_pipeline.py -k "TestSitemapAnalysis or TestFrequencyStep" -v` passes all tests.
    `cd /Users/matt/src/itsascout/scrapegrape && uv run pytest publishers/tests/test_pipeline.py -v` passes ALL tests (no regressions).
  </verify>
  <done>
    All new tests pass. run_sitemap_analysis_step correctly detects xmlns:news, handles sitemap indexes, and extracts lastmod dates. run_frequency_step computes frequency from RSS with feedparser, falls back to sitemap lastmod, and assigns correct confidence levels. No regressions in existing tests.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "import feedparser; print(feedparser.__version__)"` outputs 6.0.12
2. `cd /Users/matt/src/itsascout/scrapegrape && uv run pytest publishers/tests/test_pipeline.py -v` -- all tests pass
3. Both step functions return dicts matching the established pattern (see research output shapes)
4. Confidence thresholds: high >= 10 items + 7 days, medium >= 5 items + 3 days, low otherwise
</verification>

<success_criteria>
- feedparser 6.0.12 is a project dependency
- run_sitemap_analysis_step and run_frequency_step exist in steps.py
- Both functions follow the same pattern as run_cc_step (take Publisher, return dict, catch exceptions)
- 11+ new tests pass covering both step functions
- No regressions in existing pipeline tests
</success_criteria>

<output>
After completion, create `.planning/phases/15-content-signals/15-01-SUMMARY.md`
</output>
