---
phase: 16-google-news-readiness
plan: 02
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - scrapegrape/publishers/pipeline/supervisor.py
autonomous: true

must_haves:
  truths:
    - "Google News readiness step runs in the pipeline after metadata profile and before job completion"
    - "Step emits SSE started and completed events with step name 'google_news'"
    - "Step result is saved to resolution_job.news_signals_result"
    - "Publisher google_news_readiness flat field is updated from step result"
    - "Step errors do not fail the pipeline -- error is captured in result dict"
  artifacts:
    - path: "scrapegrape/publishers/pipeline/supervisor.py"
      provides: "Google News step wired into pipeline with SSE events and error handling"
      contains: "run_google_news_step"
  key_links:
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "scrapegrape/publishers/pipeline/steps.py"
      via: "import run_google_news_step"
      pattern: "from.*steps.*import.*run_google_news_step"
    - from: "scrapegrape/publishers/pipeline/supervisor.py"
      to: "resolution_job.news_signals_result"
      via: "save step result to JSONField"
      pattern: "news_signals_result"
---

<objective>
Wire the Google News readiness step into the pipeline supervisor with SSE events, result persistence, and non-critical error handling.

Purpose: Connects the aggregation step function (from Plan 01) into the pipeline so it runs automatically on every analysis, emits progress events, and saves results.

Output: Updated supervisor.py with Google News step after metadata profile, before pipeline completion.
</objective>

<execution_context>
@/Users/matt/.claude/get-shit-done/workflows/execute-plan.md
@/Users/matt/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-google-news-readiness/16-01-SUMMARY.md
@.planning/phases/16-google-news-readiness/16-RESEARCH.md
@.planning/phases/15-content-signals/15-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire Google News step into pipeline supervisor</name>
  <files>scrapegrape/publishers/pipeline/supervisor.py</files>
  <action>
Follow the established step wiring pattern from Phase 15-02 (see 15-02-SUMMARY.md for reference).

1. **Add import:** Add `run_google_news_step` to the existing import from `steps.py` (line ~16 area where other step imports are).

2. **Add step execution block** after the `publish_step_event(job_id, "metadata_profile", "completed", ...)` line and BEFORE the "Mark job complete" section (`resolution_job.status = "completed"`). The step must be OUTSIDE the article freshness branch (it aggregates data from both publisher-level and article-level steps).

   Insert this block:
   ```python
   # Step: Google News readiness (non-critical aggregation)
   publish_step_event(job_id, "google_news", "started")
   try:
       news_result = run_google_news_step(
           sitemap_analysis_result=resolution_job.sitemap_analysis_result,
           article_result=resolution_job.article_result,
           metadata_result=resolution_job.metadata_result,
       )
   except Exception as exc:
       logger.error(f"Google News step error for job {job_id}: {exc}")
       news_result = {
           "readiness": "",
           "signals": {},
           "signal_count": 0,
           "error": str(exc),
       }
   resolution_job.news_signals_result = news_result
   resolution_job.save(update_fields=["news_signals_result"])
   publish_step_event(job_id, "google_news", "completed", news_result)

   # Update publisher flat field
   publisher.google_news_readiness = news_result.get("readiness", "")
   publisher.save(update_fields=["google_news_readiness"])
   ```

**Important placement notes:**
- The step MUST be after the article-level steps section (it needs article_result to be populated)
- The step MUST be before "Mark job complete" (so results are saved before pipeline completion)
- The step MUST be at the same indentation level as the "Mark job complete" block (inside the outer try, but outside the article freshness if/else)
- The step has its OWN try/except so errors don't cascade to pipeline failure

3. Run `uv run pytest` to verify all existing tests pass. The integration test mocks run_pipeline, so the new step won't break it.
  </action>
  <verify>`uv run pytest` -- full test suite passes. Grep supervisor.py for `run_google_news_step` to confirm import and usage present.</verify>
  <done>Google News step is wired into pipeline after metadata profile, before completion. SSE events emit for google_news started/completed. Result saved to news_signals_result. Publisher flat field updated. Errors caught without failing pipeline. All tests pass.</done>
</task>

</tasks>

<verification>
1. `uv run pytest` -- all tests pass
2. `grep -n "run_google_news_step" scrapegrape/publishers/pipeline/supervisor.py` -- shows import and usage
3. `grep -n "google_news" scrapegrape/publishers/pipeline/supervisor.py` -- shows SSE events
4. `grep -n "news_signals_result" scrapegrape/publishers/pipeline/supervisor.py` -- shows result persistence
5. Verify step is AFTER metadata_profile completed event and BEFORE status = "completed"
</verification>

<success_criteria>
- run_google_news_step imported and called in supervisor
- SSE events emitted: google_news started + completed
- Result saved to resolution_job.news_signals_result
- Publisher.google_news_readiness flat field updated
- Non-critical: step errors produce error dict, not pipeline failure
- All existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/16-google-news-readiness/16-02-SUMMARY.md`
</output>
